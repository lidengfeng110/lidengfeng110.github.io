{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"HAproxy 原理介绍","slug":"HAproxy 原理介绍","date":"2017-08-27T16:00:00.000Z","updated":"2018-08-09T08:28:45.910Z","comments":true,"path":"2017/08/28/HAproxy 原理介绍/","link":"","permalink":"http://yoursite.com/2017/08/28/HAproxy 原理介绍/","excerpt":"课前扩展：运维人员的三大核心工作： 发布； 变更； 故障处理； 扩展： 向上，向外。 什么是无状态跟有状态？ 一个客户端向服务器发起多次请求，后一次的请求跟前一次的请求是隔离的，独立的，相互没有关系的，这就叫无状态。 如果后一次的请求必须建立在前一次请求之上，这种就叫做有状态请求。 HAproxy 是一个代理服务器，但他天然是一个能在代理时做负载均衡调度的服务器两个特点： 反代 mode http ： 七层反代 要根据用户的资源请求做分离跳读 ssl/tls 会话卸载器 mode tcp： 伪四层反代 为什么说是伪四层呢，因为它依旧受限于七层的并发连接数。 调度器 支持众多的调度算法 如：轮询，加权轮询等；","text":"课前扩展：运维人员的三大核心工作： 发布； 变更； 故障处理； 扩展： 向上，向外。 什么是无状态跟有状态？ 一个客户端向服务器发起多次请求，后一次的请求跟前一次的请求是隔离的，独立的，相互没有关系的，这就叫无状态。 如果后一次的请求必须建立在前一次请求之上，这种就叫做有状态请求。 HAproxy 是一个代理服务器，但他天然是一个能在代理时做负载均衡调度的服务器两个特点： 反代 mode http ： 七层反代 要根据用户的资源请求做分离跳读 ssl/tls 会话卸载器 mode tcp： 伪四层反代 为什么说是伪四层呢，因为它依旧受限于七层的并发连接数。 调度器 支持众多的调度算法 如：轮询，加权轮询等； HAProxy：HAproxy 也使用事件驱动模型，单进程来相应多请求的模式工作。 但是建议工作在单进程模式下，足以提供交大并发的请求，这样更容易排查和定义故障问题。 http://www.haproxy.org http://www.haproxy.com 文档： http://cbonte.github.io/haproxy-dconv/ HAProxy is a TCP/HTTP reverse proxy which is particularly suited for high availability environments. Indeed, it can: : - route HTTP requests depending on statically assigned cookies # 是一个 HTTP 的路由器，能够把 HTTP 请求路由至最佳节点，还支持做静态 COOKIES 绑定以后做会话连线。 : - spread load among several servers while assuring server persistence # 能否实现调度 : through the use of HTTP cookies # 可以实现对于 HTTP cookies 的高效利用 : - switch to backup servers in the event a main server fails # 在主服务器宕机是可启用备用服务器，通常叫做 抱歉服务器 : - accept connections to special ports dedicated to service monitoring : - stop accepting connections without breaking existing ones : - add, modify, and delete HTTP headers in both directions : - block requests matching particular patterns : - report detailed status to authenticated users from a URI intercepted by the application 版本：1.4, 1.5, 1.6, 1.7 以后不管搭建什么服务，时间一定都要同步，很关键也很重要。 同步时间有一个命令： timedatectl set-timezone Asia/Shanghai 搭建架构时，在所有架构中的服务器上都执行这个命令。 程序环境： 主程序：/usr/sbin/haproxy 主配置文件：/etc/haproxy/haproxy.cfg Unit file：/usr/lib/systemd/system/haproxy.service 配置段： global：全局配置段 进程及安全配置相关的参数 性能调整相关参数 Debug参数 用户列表 peers # 同等端点的兄弟服务器的位置跟通讯方法 proxies：代理配置段 defaults：为frontend, listen, backend提供默认配置； fronted：前端，相当于nginx, server {} backend：后端，相当于nginx, upstream {} listen：同时拥前端和后端 简单的配置示例： frontend web bind *:80 default_backend websrvs backend websrvs balance roundrobin server srv1 192.168.1.132:80 check weight 2 server srv2 192.168.1.134:80 check IaaS, PaaS, SaaS LBaaS, DBaaS, FWaaS, FaaS(Serverless), … OpenShift(PaaS): HAPorxy, Ingress Controller global配置参数： 进程及安全管理：chroot, daemon，user, group, uid, gid log：定义全局的syslog服务器；最多可以定义两个； log &lt;address&gt; [len &lt;length&gt;] &lt;facility&gt; [max level [min level]] log &lt;address&gt; :表示日志发送给哪个 syslog 服务器 [len &lt;length&gt;] ：记录日志的最大长度是多长 &lt;facility&gt; [max level [min level]] : 最大日志级别跟最低日志级别 如何启用日志功能？ 在 hapeoxy 配置文件中复制日志格式到 /etc/syslog.conf 中的对应的语句块中。 如： 复制 haproxy 配置文件中的 local2.* /var/log/haproxy.log 到 /etc/syslog.conf 文件中对应的语句块中，并重启服务 最后客户端的访问日志就会记录在 /var/log/haproxy.log 文件中 nbproc &lt;number&gt;：要启动的haproxy的进程数量； ulimit-n &lt;number&gt;：每个haproxy进程可打开的最大文件数； 性能调整： maxconn &lt;number&gt;：设定每个haproxy进程所能接受的最大并发连接数；Sets the maximum per-process number of concurrent connections to &lt;number&gt;. 总体的并发连接数：nbproc * maxconn maxconnrate &lt;number&gt;：Sets the maximum per-process number of connections per second to &lt;number&gt;. 每个进程每秒种所能创建的最大连接数量； maxsessrate &lt;number&gt;： 每个进程每秒钟能够创建的会话速率 maxsslconn &lt;number&gt;: Sets the maximum per-process number of concurrent SSL connections to &lt;number&gt;. 设定每个haproxy进程所能接受的ssl的最大并发连接数； spread-checks &lt;0..50, in percent&gt; 代理配置段： - defaults &lt;name&gt; - frontend &lt;name&gt; - backend &lt;name&gt; - listen &lt;name&gt; A &quot;frontend&quot; section describes a set of listening sockets accepting client connections. A &quot;backend&quot; section describes a set of servers to which the proxy will connect to forward incoming connections. A &quot;listen&quot; section defines a complete proxy with its frontend and backend parts combined in one section. It is generally useful for TCP-only traffic. All proxy names must be formed from upper and lower case letters, digits, &apos;-&apos; (dash), &apos;_&apos; (underscore) , &apos;.&apos; (dot) and &apos;:&apos; (colon). 区分字符大小写； 配置参数： bind：Define one or several listening addresses and/or ports in a frontend. bind [&lt;address&gt;]:&lt;port_range&gt; [, ...] [param*] listen http_proxy bind :80,:443 bind 10.0.0.1:10080,10.0.0.1:10443 bind /var/run/ssl-frontend.sock user root mode 600 accept-proxy # 这个参数只能用在 frontend 和 listen 语句块中。 balance：后端服务器组内的服务器调度算法 balance &lt;algorithm&gt; [ &lt;arguments&gt; ] balance url_param &lt;param&gt; [check_post] 算法： roundrobin：Each server is used in turns, according to their weights. server options： weight # 动态算法：支持权重的运行时调整，支持慢启动；每个后端中最多支持4095个server； static-rr： 静态算法：不支持权重的运行时调整及慢启动；后端主机数量无上限； leastconn： 推荐使用在具有较长会话的场景中，例如MySQL、LDAP等； first： 根据服务器在列表中的位置，自上而下进行调度；前面服务器的连接数达到上限，新请求才会分配给下一台服务； source：源地址hash； 将同一个地址发过来的请求一直发往一个后端服务器 除权取余法（静态数组取模法）： 一致性哈希： uri： 对URI的左半部分做hash计算，并由服务器总权重相除以后派发至某挑出的服务器； # 如果在生产中，调度器后面是缓存服务器，uri 就要用一致性哈希的动态算法。 &lt;scheme&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;;&lt;params&gt;?&lt;query&gt;#&lt;frag&gt; 左半部分：/&lt;path&gt;;&lt;params&gt; 整个uri：/&lt;path&gt;;&lt;params&gt;?&lt;query&gt;#&lt;frag&gt; username=jerry url_param：对用户请求的uri的&lt;params&gt;部分中的参数的值作hash计算，并由服务器总权重相除以后派发至某挑出的服务器；通常用于追踪用户，以确保来自同一个用户的请求始终发往同一个Backend Server； hdr(&lt;name&gt;)：对于每个http请求，此处由&lt;name&gt;指定的http首部将会被取出做hash计算； 并由服务器总权重相除以后派发至某挑出的服务器；没有有效值的会被轮询调度； hdr(Cookie) rdp-cookie rdp-cookie(&lt;name&gt;) hash-type：哈希算法 hash-type &lt;method&gt; &lt;function&gt; &lt;modifier&gt; map-based：除权取余法，哈希数据结构是静态的数组； consistent：一致性哈希，哈希数据结构是一个树； &lt;function&gt; is the hash function to be used : 哈希函数 sdbm djb2 wt6 default_backend &lt;backend&gt; 设定默认的backend，用于frontend中； default-server [param*] 为backend中的各server设定默认选项； server &lt;name&gt; &lt;address&gt;[:[port]] [param*] 定义后端主机的各服务器及其选项； server &lt;name&gt; &lt;address&gt;[:port] [settings ...] default-server [settings ...] &lt;name&gt;：服务器在haproxy上的内部名称；出现在日志及警告信息中； &lt;address&gt;：服务器地址，支持使用主机名； [:[port]]：端口映射；省略时，表示同bind中绑定的端口； [param*]：参数 maxconn &lt;maxconn&gt;：当前server的最大并发连接数； backlog &lt;backlog&gt;：当前server的连接数达到上限后的后援队列长度； backup：设定当前server为备用服务器； check：对当前server做健康状态检测； addr ：检测时使用的IP地址； port ：针对此端口进行检测； inter &lt;delay&gt;：连续两次检测之间的时间间隔，默认为2000ms; rise &lt;count&gt;：连续多少次检测结果为“成功”才标记服务器为可用；默认为2； fall &lt;count&gt;：连续多少次检测结果为“失败”才标记服务器为不可用；默认为3； 注意：option httpchk，&quot;smtpchk&quot;, &quot;mysql-check&quot;, &quot;pgsql-check&quot; and &quot;ssl-hello-chk&quot; 用于定义应用层检测方法； cookie &lt;value&gt;：为当前server指定其cookie值，用于实现基于cookie的会话黏性； disabled：标记为不可用； on-error &lt;mode&gt;：后端服务故障时的行动策略； - fastinter: force fastinter - fail-check: simulate a failed check, also forces fastinter (default) - sudden-death: simulate a pre-fatal failed health check, one more failed check will mark a server down, forces fastinter - mark-down: mark the server immediately down and force fastinter redir &lt;prefix&gt;：将发往此server的所有GET和HEAD类的请求重定向至指定的URL； weight &lt;weight&gt;：权重，默认为1; OK --&gt; PROBLEM OK --&gt; PROBLEM --&gt; PROBLEM --&gt; PROBLEM PROBLEM --&gt; OK 统计接口启用相关的参数： stats enable (加在前端后端都可以) 启用统计页；基于默认的参数启用stats page； - stats uri : /haproxy?stats - stats realm : &quot;HAProxy Statistics&quot; - stats auth : no authentication - stats scope : no restriction stats auth &lt;user&gt;:&lt;passwd&gt; 认证时的账号和密码，可使用多次；（启用账号密码） stats realm &lt;realm&gt; 认证时的realm；（提示语） stats uri &lt;prefix&gt; 自定义stats page uri stats refresh &lt;delay&gt; 设定自动刷新时间间隔； stats admin { if | unless } &lt;cond&gt; 启用stats page中的管理功能 配置示例：（为了信息安全，可以专门给状态信息加端口） listen stats bind :9099 stats enable stats realm HAPorxy\\ Stats\\ Page stats auth admin:admin stats admin if TRUE maxconn &lt;conns&gt;：为指定的frontend定义其最大并发连接数；默认为2000； Fix the maximum number of concurrent connections on a frontend. mode { tcp|http|health } 定义haproxy的工作模式； tcp：基于layer4实现代理；可代理mysql, pgsql, ssh, ssl等协议； http：仅当代理的协议为http时使用； health：工作为健康状态检查的响应模式，当连接请求到达时回应“OK”后即断开连接； 示例： listen ssh bind :22022 balance leastconn mode tcp server sshsrv1 172.16.100.6:22 check server sshsrv2 172.16.100.7:22 check cookie &lt;name&gt; [ rewrite | insert | prefix ] [ indirect ] [ nocache ] [ postonly ] [ preserve ] [ httponly ] [ secure ] [ domain &lt;domain&gt; ]* [ maxidle &lt;idle&gt; ] [ maxlife &lt;life&gt; ] &lt;name&gt;：is the name of the cookie which will be monitored, modified or inserted in order to bring persistence. rewirte：重写； insert：插入； prefix：前缀； 基于cookie的session sticky的实现： backend websrvs cookie WEBSRV insert nocache indirect server srv1 172.16.100.6:80 weight 2 check rise 1 fall 2 maxconn 3000 cookie srv1 server srv2 172.16.100.7:80 weight 1 check rise 1 fall 2 maxconn 3000 cookie srv2 option forwardfor [ except &lt;network&gt; ] [ header &lt;name&gt; ] [ if-none ] Enable insertion of the X-Forwarded-For header to requests sent to servers 在由haproxy发往后端主机的请求报文中添加“X-Forwarded-For”首部，其值前端客户端的地址；用于向后端主发送真实的客户端IP； [ except &lt;network&gt; ]：请求报请来自此处指定的网络时不予添加此首部； [ header &lt;name&gt; ]：使用自定义的首部名称，而非“X-Forwarded-For”； errorfile &lt;code&gt; &lt;file&gt; Return a file contents instead of errors generated by HAProxy &lt;code&gt;：is the HTTP status code. Currently, HAProxy is capable of generating codes 200, 400, 403, 408, 500, 502, 503, and 504. &lt;file&gt;：designates a file containing the full HTTP response. 示例： errorfile 400 /etc/haproxy/errorfiles/400badreq.http errorfile 408 /dev/null # workaround Chrome pre-connect bug errorfile 403 /etc/haproxy/errorfiles/403forbid.http errorfile 503 /etc/haproxy/errorfiles/503sorry.http errorloc &lt;code&gt; &lt;url&gt; errorloc302 &lt;code&gt; &lt;url&gt; errorfile 403 http://www.magedu.com/error_pages/403.html reqadd &lt;string&gt; [{if | unless} &lt;cond&gt;] Add a header at the end of the HTTP request rspadd &lt;string&gt; [{if | unless} &lt;cond&gt;] Add a header at the end of the HTTP response # 给客户端法响应报文时给报文头部添加信息 rspadd X-Via:\\ HAPorxy reqdel &lt;search&gt; [{if | unless} &lt;cond&gt;] reqidel &lt;search&gt; [{if | unless} &lt;cond&gt;] (ignore case) Delete all headers matching a regular expression in an HTTP request rspdel &lt;search&gt; [{if | unless} &lt;cond&gt;] rspidel &lt;search&gt; [{if | unless} &lt;cond&gt;] (ignore case) Delete all headers matching a regular expression in an HTTP response rspidel Server.* # 给客户端发送响应报文时可以删除报文头部的信息 日志系统： log： log global log &lt;address&gt; [len &lt;length&gt;] &lt;facility&gt; [&lt;level&gt; [&lt;minlevel&gt;]] no log 注意： 默认发往本机的日志服务器； (1) local2.* /var/log/local2.log (2) $ModLoad imudp $UDPServerRun 514 log-format &lt;string&gt;： 课外实践：参考文档实现combined格式的记录 capture cookie &lt;name&gt; len &lt;length&gt; Capture and log a cookie in the request and in the response. capture request header &lt;name&gt; len &lt;length&gt; Capture and log the last occurrence of the specified request header. capture request header X-Forwarded-For len 15 capture response header &lt;name&gt; len &lt;length&gt; Capture and log the last occurrence of the specified response header. capture response header Content-length len 9 capture response header Location len 15 为指定的MIME类型启用压缩传输功能 compression algo &lt;algorithm&gt; ...：启用http协议的压缩机制，指明压缩算法gzip, deflate； compression type &lt;mime type&gt; ...：指明压缩的MIME类型；常适用于压缩的类型为文本类型； 对后端服务器做http协议的健康状态检测： option httpchk option httpchk &lt;uri&gt; option httpchk &lt;method&gt; &lt;uri&gt; option httpchk &lt;method&gt; &lt;uri&gt; &lt;version&gt; 定义基于http协议的7层健康状态检测机制； http-check expect [!] &lt;match&gt; &lt;pattern&gt; Make HTTP health checks consider response contents or specific status codes. 连接超时时长： timeout client &lt;timeout&gt; # 面向客户端的超时时间 Set the maximum inactivity time on the client side. 默认单位是毫秒; timeout server &lt;timeout&gt; # 面向服务器端的超时时间 Set the maximum inactivity time on the server side. timeout http-keep-alive &lt;timeout&gt; 持久连接的持久时长； timeout http-request &lt;timeout&gt; Set the maximum allowed time to wait for a complete HTTP request timeout connect &lt;timeout&gt; Set the maximum time to wait for a connection attempt to a server to succeed. timeout client-fin &lt;timeout&gt; Set the inactivity timeout on the client side for half-closed connections. timeout server-fin &lt;timeout&gt; Set the inactivity timeout on the server side for half-closed connections. use_backend &lt;backend&gt; [{if | unless} &lt;condition&gt;] Switch to a specific backend if/unless an ACL-based condition is matched. 当符合指定的条件时使用特定的backend； block { if | unless } &lt;condition&gt; # 阻止谁来访问 Block a layer 7 request if/unless a condition is matched acl invalid_src src 172.16.200.2 block if invalid_src errorfile 403 /etc/fstab http-request { allow | deny } [ { if | unless } &lt;condition&gt; ] Access control for Layer 7 requests tcp-request connection {accept|reject} [{if | unless} &lt;condition&gt;] Perform an action on an incoming connection depending on a layer 4 condition 示例： listen ssh bind :22022 balance leastconn acl invalid_src src 172.16.200.2 tcp-request connection reject if invalid_src mode tcp server sshsrv1 172.16.100.6:22 check server sshsrv2 172.16.100.7:22 check backup acl： The use of Access Control Lists (ACL) provides a flexible solution to perform content switching and generally to take decisions based on content extracted from the request, the response or any environmental status. acl &lt;aclname&gt; &lt;criterion&gt; [flags] [operator] [&lt;value&gt;] ... &lt;aclname&gt;：ACL names must be formed from upper and lower case letters, digits, &apos;-&apos; (dash), &apos;_&apos; (underscore) , &apos;.&apos; (dot) and &apos;:&apos; (colon).ACL names are case-sensitive. &lt;value&gt;的类型： - boolean - integer or integer range - IP address / network - string (exact, substring, suffix, prefix, subdir, domain) - regular expression （以正则表达式做匹配） - hex block （以 16 进制的代码块做匹配） &lt;flags&gt; -i : ignore case during matching of all subsequent patterns. # 不区分字符大小写 -m : use a specific pattern matching method -n : forbid the DNS resolutions （禁止 DNS 做名字解析） -u : force the unique id of the ACL （要求 ACL必须使用唯一的名称） -- : force end of flags. Useful when a string looks like one of the flags. [operator] 匹配整数值：eq、ge、gt、le、lt 匹配字符串： - exact match (-m str) : the extracted string must exactly match the patterns ; - substring match (-m sub) : the patterns are looked up inside the extracted string, and the ACL matches if any of them is found inside ; - prefix match (-m beg) : the patterns are compared with the beginning of the extracted string, and the ACL matches if any of them matches. - suffix match (-m end) : the patterns are compared with the end of the extracted string, and the ACL matches if any of them matches. - subdir match (-m dir) : the patterns are looked up inside the extracted string, delimited with slashes (&quot;/&quot;), and the ACL matches if any of them matches. - domain match (-m dom) : the patterns are looked up inside the extracted string, delimited with dots (&quot;.&quot;), and the ACL matches if any of them matches. acl作为条件时的逻辑关系： - AND (implicit) - OR (explicit with the &quot;or&quot; keyword or the &quot;||&quot; operator) - Negation with the exclamation mark (&quot;!&quot;) if invalid_src invalid_port # 并且的关系，两个都得满足 if invalid_src || invalid_port # 或者的关系，有一个满足就行 if ! invalid_src invalid_port # 表示不满足第一个，但满足第二个就行 &lt;criterion&gt; ： dst : ip dst_port : integer src : ip src_port : integer acl invalid_src src 172.16.200.2 path : string This extracts the request&apos;s URL path, which starts at the first slash and ends before the question mark (without the host part). /path;&lt;params&gt; path : exact string match path_beg : prefix match # 前缀匹配 path_dir : subdir match # 路径子串匹配 path_dom : domain match # 子域名匹配 path_end : suffix match # 后缀匹配 path_len : length match # 路径的长度匹配 path_reg : regex match # 路径的正则表达式匹配 path_sub : substring match # 子串匹配 path_beg /images/ path_end .jpg .jpeg .png .gif path_reg ^/images.*\\.jpeg$ path_sub image path_dir jpegs path_dom ilinux /images/jpegs/20180312/logo.jpg url : string This extracts the request&apos;s URL as presented in the request. A typical use is with prefetch-capable caches, and with portals which need to aggregate multiple information from databases and keep them in caches. url : exact string match url_beg : prefix match url_dir : subdir match url_dom : domain match url_end : suffix match url_len : length match url_reg : regex match url_sub : substring match req.hdr([&lt;name&gt;[,&lt;occ&gt;]]) : string This extracts the last occurrence of header &lt;name&gt; in an HTTP request. hdr([&lt;name&gt;[,&lt;occ&gt;]]) : exact string match hdr_beg([&lt;name&gt;[,&lt;occ&gt;]]) : prefix match hdr_dir([&lt;name&gt;[,&lt;occ&gt;]]) : subdir match hdr_dom([&lt;name&gt;[,&lt;occ&gt;]]) : domain match hdr_end([&lt;name&gt;[,&lt;occ&gt;]]) : suffix match hdr_len([&lt;name&gt;[,&lt;occ&gt;]]) : length match hdr_reg([&lt;name&gt;[,&lt;occ&gt;]]) : regex match hdr_sub([&lt;name&gt;[,&lt;occ&gt;]]) : substring match 示例： acl bad_curl hdr_sub(User-Agent) -i curl block if bad_curl status : integer Returns an integer containing the HTTP status code in the HTTP response. Pre-defined ACLs ACL name Equivalent to Usage FALSE always_false never match HTTP req_proto_http match if protocol is valid HTTP HTTP_1.0 req_ver 1.0 match HTTP version 1.0 HTTP_1.1 req_ver 1.1 match HTTP version 1.1 HTTP_CONTENT hdr_val(content-length) gt 0 match an existing content-length HTTP_URL_ABS url_reg ^[^/:]*:// match absolute URL with scheme HTTP_URL_SLASH url_beg / match URL beginning with &quot;/&quot; HTTP_URL_STAR url * match URL equal to &quot;*&quot; LOCALHOST src 127.0.0.1/8 match connection from local host METH_CONNECT method CONNECT match HTTP CONNECT method METH_GET method GET HEAD match HTTP GET or HEAD method METH_HEAD method HEAD match HTTP HEAD method METH_OPTIONS method OPTIONS match HTTP OPTIONS method METH_POST method POST match HTTP POST method METH_TRACE method TRACE match HTTP TRACE method RDP_COOKIE req_rdp_cookie_cnt gt 0 match presence of an RDP cookie REQ_CONTENT req_len gt 0 match data in the request buffer TRUE always_true always match WAIT_END wait_end wait for end of content analysis HAProxy：global, proxies（fronted, backend, listen, defaults） balance： roundrobin, static-rr leastconn first source hdr(&lt;name&gt;) uri (hash-type) url_param Nginx调度算法：ip_hash, hash, leastconn, lvs调度算法： rr/wrr/sh/dh, lc/wlc/sed/nq/lblc/lblcr 基于ACL的动静分离示例： frontend web *:80 acl url_static path_beg -i /static /images /javascript /stylesheets acl url_static path_end -i .jpg .gif .png .css .js .html .txt .htm use_backend staticsrvs if url_static default_backend appsrvs backend staticsrvs balance roundrobin server stcsrv1 172.16.100.6:80 check backend appsrvs balance roundrobin server app1 172.16.100.7:80 check server app1 172.16.100.7:8080 check listen stats bind :9091 stats enable stats auth admin:admin stats admin if TRUE 配置HAProxy支持https协议： 1 支持ssl会话； bind *:443 ssl crt /PATH/TO/SOME_PEM_FILE crt后的证书文件要求PEM格式，且同时包含证书和与之匹配的所有私钥； cat demo.crt demo.key &gt; demo.pem 2 把80端口的请求重向定443； bind *:80 redirect scheme https if !{ ssl_fc } 另一种配置：对非ssl的任何url的访问统统定向至https主机的主页； redirect location https://172.16.0.67/ if !{ ssl_fc } 3 如何向后端传递用户请求的协议和端口 http_request set-header X-Forwarded-Port %[dst_port] http_request add-header X-Forwared-Proto https if { ssl_fc } 配置时常用的功能： http --&gt; https mode http 压缩、条件式转发、算法、stats page、自定义错误页、访问控制、日志功能 最大并发连接； global, defaults, frontend, listen, server 基于cookie的session粘滞 后端主机的健康状态检测 请求和响应报文首部的操纵 实践（博客）作业： http: (1) 动静分离部署wordpress，动静都要能实现负载均衡，要注意会话的问题； (2) 在haproxy和后端主机之间添加varnish进行缓存； (3) 给出设计拓扑，写成博客； (4) haproxy的设定要求： (a) stats page，要求仅能通过本地访问使用管理接口； (b) 动静分离； (c) 分别考虑不同的服务器组的调度算法； (4) 压缩合适的内容类型；","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"haproxy","slug":"haproxy","permalink":"http://yoursite.com/tags/haproxy/"}]},{"title":"ext 文件系统机制原理","slug":"ext 文件系统机制原理","date":"2017-04-19T16:00:00.000Z","updated":"2018-06-25T11:32:23.414Z","comments":true,"path":"2017/04/20/ext 文件系统机制原理/","link":"","permalink":"http://yoursite.com/2017/04/20/ext 文件系统机制原理/","excerpt":"将磁盘进行分区，分区是将磁盘按柱面进行物理上的划分。划分好分区后还要进行格式化，然后再挂载才能使用(不考虑其他方法)。格式化分区的过程其实就是创建文件系统。 文件系统的类型有很多种，如CentOS 5和CentOS 6上默认使用的ext2/ext3/ext4，CentOS 7上默认使用的xfs，windows上的NTFS，光盘类的文件系统ISO9660，MAC上的混合文件系统HFS，网络文件系统NFS，Oracle研发的btrfs，还有老式的FAT/FAT32等。 本文将非常全面且详细地对ext家族的文件系统进行介绍。有ext2/ext3/ext4，ext3是有日志的ext2改进版，ext4对相比ext3做了非常多的改进。虽然xfs/btrfs等文件系统有所不同，但它们只是在实现方式上不太同，再加上属于自己的特性而已。","text":"将磁盘进行分区，分区是将磁盘按柱面进行物理上的划分。划分好分区后还要进行格式化，然后再挂载才能使用(不考虑其他方法)。格式化分区的过程其实就是创建文件系统。 文件系统的类型有很多种，如CentOS 5和CentOS 6上默认使用的ext2/ext3/ext4，CentOS 7上默认使用的xfs，windows上的NTFS，光盘类的文件系统ISO9660，MAC上的混合文件系统HFS，网络文件系统NFS，Oracle研发的btrfs，还有老式的FAT/FAT32等。 本文将非常全面且详细地对ext家族的文件系统进行介绍。有ext2/ext3/ext4，ext3是有日志的ext2改进版，ext4对相比ext3做了非常多的改进。虽然xfs/btrfs等文件系统有所不同，但它们只是在实现方式上不太同，再加上属于自己的特性而已。 文件系统的组成部分block的出现硬盘的读写IO一次是一个扇区512字节，如果要读写大量文件，以扇区为单位肯定很慢很消耗性能，所以Linux中通过文件系统控制使用”块”为读写单元。现在的文件系统上，块的大小一般为1024bytes(1K)或2048bytes(2K)或4096bytes(4K)。比如需要读一个或多个块时，文件系统的IO管理器通知磁盘控制器要读取哪些块的数据，硬盘控制器将这些块按扇区读取出来，再通过硬盘控制器将这些扇区数据重组返回给计算机。 block的出现使得在文件系统层面上读写性能大大提高，也大量减少了碎片。但是它的副作用是可能造成空间浪费。由于文件系统以block为读写单元，即使存储的文件只有1K大小也将占用一个block，剩余的空间完全是浪费的。在某些业务需求下可能大量存储小文件，这会浪费大量的空间。 尽管有缺点，但是其优点足够明显，在当下硬盘容量廉价且追求性能的时代，使用block是一定的。 inode的出现如果存储的1个文件占用了大量的block读取时会如何？假如block大小为1KB，仅仅存储一个10M的文件就需要10240个block，而且这些blocks很可能在位置上是不连续在一起的(不相邻)，读取该文件时难道要从前向后扫描整个文件系统的块，然后找出属于该文件的块吗？显然是不应该这么做的，因为太慢太傻瓜式了。再考虑一下，读取一个只占用1个block的文件，难道只读取一个block就结束了吗？并不是，仍然是扫描整个文件系统的所有block，因为它不知道什么时候扫描到，扫描到了它也不知道这个文件是不是已经完整而不需要再扫描其他的block。 另外，每个文件都有属性(如权限、大小、时间戳等)，这些属性类的元数据存储在哪里呢？难道也和文件的数据部分存储在块中吗？如果一个文件占用多个block那是不是每个属于该文件的block都要存储一份文件元数据？但是如果不在每个block中存储元数据文件系统又怎么知道某一个block是不是属于该文件呢？但是显然，每个数据block中都存储一份元数据太浪费空间。 文件系统设计者当然知道这样的存储方式很不理想，所以需要优化存储方式。如何优化？对于这种类似的问题的解决方法是使用索引，通过扫描索引找到对应的数据，而且索引可以存储部分数据。 在文件系统上索引技术具体化为索引节点(index node)，在索引节点上存储的部分数据即为文件的属性元数据及其他少量信息。一般来说索引占用的空间相比其索引的文件数据而言占用的空间就小得多，扫描它比扫描整个数据要快得多，否则索引就没有存在的意义。这样一来就解决了前面所有的问题。 在文件系统上的术语中，索引节点称为inode。在inode中存储了inode号、文件类型、权限、文件所有者、大小、时间戳等元数据信息，最重要的是还存储了指向属于该文件block的指针，这样读取inode就可以找到属于该文件的block，进而读取这些block并获得该文件的数据。由于后面还会介绍一种指针，为了方便称呼和区分，暂且将这个inode记录中指向文件data block的指针称之为block指针，。 一般inode大小为128字节或256字节，相比那些MB或GB计算的文件数据而言小得多的多，但也要知道可能一个文件大小小于inode大小，例如只占用1个字节的文件。 bmap出现在向硬盘存储数据时，文件系统需要知道哪些块是空闲的，哪些块是已经占用了的。最笨的方法当然是从前向后扫描，遇到空闲块就存储一部分，继续扫描直到存储完所有数据。 优化的方法当然也可以考虑使用索引，但是仅仅1G的文件系统就有1KB的block共1024*1024=1048576个，这仅仅只是1G，如果是100G、500G甚至更大呢，仅仅使用索引索引的数量和空间占用也将极大，这时就出现更高一级的优化方法：使用块位图(bitmap简称bmap)。 位图只使用0和1标识对应block是空闲还是被占用，0和1在位图中的位置和block的位置一一对应，第一位标识第一个块，第二个位标识第二个块，依次下去直到标记完所有的block。 考虑下为什么块位图更优化。在位图中1个字节8个位，可以标识8个block。对于一个block大小为1KB、容量为1G的文件系统而言，block数量有10241024个，所以在位图中使用10241024个位共1024*1024/8=131072字节=128K，即1G的文件只需要128个block做位图就能完成一一对应。通过扫描这100多个block就能知道哪些block是空闲的，速度提高了非常多。 但是要注意，bmap的优化针对的是写优化，因为只有写才需要找到空闲block并分配空闲block。对于读而言，只要通过inode找到了block的位置，cpu就能迅速计算出block在物理磁盘上的地址，cpu的计算速度是极快的，计算block地址的时间几乎可以忽略，那么读速度基本认为是受硬盘本身性能的影响而与文件系统无关。大多数稍大一点的文件可能都会存储在不连续的block上，而且使用了一段时间的文件系统可能会有不少碎片，这时硬盘的随机读取性能直接决定读数据的速度，这也是机械硬盘速度相比固态硬盘慢的多的多的原因之一，而且固态硬盘的随机读和连续读取速度几乎是一致的，对它来说，文件系统碎片的多少并不会影响读取速度。 虽然bmap已经极大的优化了扫描，但是仍有其瓶颈：如果文件系统是100G呢？100G的文件系统要使用128*100=12800个1KB大小的block，这就占用了12.5M的空间了。试想完全扫描12800个很可能不连续的block这也是需要占用一些时间的，虽然快但是扛不住每次存储文件都要扫描带来的巨大开销。 所以需要再次优化，如何优化？简而言之就是将文件系统划分开形成块组，至于块组的介绍放在后文。 inode表的出现回顾下inode相关信息：inode存储了inode号、文件属性元数据、指向文件占用的block的指针；每一个inode占用128字节或256字节。 现在又出现问题了，一个文件系统中可以说有无数多个文件，每一个文件都对应一个inode，难道每一个仅128字节的inode都要单独占用一个block进行存储吗？这太浪费空间了。 所以更优的方法是将多个inode合并存储在block中，对于128字节的inode，一个block存储8个inode，对于256字节的inode，一个block存储4个inode。这就使得每个存储inode的块都不浪费。 在ext文件系统上，将这些物理上存储inode的block组合起来，在逻辑上形成一张inode表(inode table)来记录所有的inode。 举个例子，每一个家庭都要向派出所登记户口信息，通过户口本可以知道家庭住址，而每个镇或街道的派出所将本镇或本街道的所有户口整合在一起，要查找某一户地址时，在派出所就能快速查找到。inode table就是这里的派出所。它的内容如下图所示。 实际上，在文件系统创建完成后所有的inode号都已经分配好并记录到inode table中了，只不过被使用的inode号所在的行还有文件属性的元数据信息和block位置信息，而未被使用的inode号只有一个inode号而已而没有其他信息而已。 再细细一思考，就能发现一个大的文件系统仍将占用大量的块来存储inode，想要找到其中的一个inode记录也需要不小的开销，尽管它们已经形成了一张逻辑上的表，但扛不住表太大记录太多。那么如何快速找到inode，这同样是需要优化的，优化的方法是将文件系统的block进行分组划分，每个组中都存有本组inode table范围、bmap等。 imap的出现前面说bmap是块位图，用于标识文件系统中哪些block是空闲哪些block是占用的。 对于inode也一样，在存储文件(Linux中一切皆文件)时需要为其分配一个inode号。但是在格式化创建文件系统后所有的inode号都是被事先设定好存放在inode table中的，因此产生了问题：要为文件分配哪一个inode号呢？又如何知道某一个inode号是否已经被分配了呢？ 既然是”是否被占用”的问题，使用位图是最佳方案，像bmap记录block的占用情况一样。标识inode号是否被分配的位图称为inodemap简称为imap。这时要为一个文件分配inode号只需扫描imap即可知道哪一个inode号是空闲的。 imap存在着和bmap和inode table一样需要解决的问题：如果文件系统比较大，imap本身就会很大，每次存储文件都要进行扫描，会导致效率不够高。同样，优化的方式是将文件系统占用的block划分成块组，每个块组有自己的imap范围。 块组的出现前面一直提到的优化方法是将文件系统占用的block划分成块组(block group)，解决bmap、inode table和imap太大的问题。 在物理层面上的划分是将磁盘按柱面划分为多个分区，即多个文件系统；在逻辑层面上的划分是将文件系统划分成块组。每个文件系统包含多个块组，每个块组包含多个元数据区和数据区：元数据区就是存储bmap、inode table、imap等的数据；数据区就是存储文件数据的区域。注意块组是逻辑层面的概念，所以并不会真的在磁盘上按柱面、按扇区、按磁道等概念进行划分。 块组的划分块组在文件系统创建完成后就已经划分完成了，也就是说元数据区bmap、inode table和imap等信息占用的block以及数据区占用的block都已经划分好了。那么文件系统如何知道一个块组元数据区包含多少个block，数据区又包含多少block呢？ 它只需确定一个数据——每个block的大小，再根据bmap至多只能占用一个完整的block的标准就能计算出块组如何划分。如果文件系统非常小，所有的bmap总共都不能占用完一个block，那么也只能空闲bmap的block了。 每个block的大小在创建文件系统时可以人为指定，不指定也有默认值。 假如现在block的大小是1KB，一个bmap完整占用一个block能标识1024*8= 8192个block(当然这8192个block是数据区和元数据区共8192个，因为元数据区分配的block也需要通过bmap来标识)。每个block是1K，每个块组是8192K即8M，创建1G的文件系统需要划分1024/8=128个块组，如果是1.1G的文件系统呢？128+12.8=128+13=141个块组。 每个组的block数目是划分好了，但是每个组设定多少个inode号呢？inode table占用多少block呢？这需要由系统决定了，因为描述”每多少个数据区的block就为其分配一个inode号”的指标默认是我们不知道的，当然创建文件系统时也可以人为指定这个指标或者百分比例。见后文”inode深入”。 使用dumpe2fs可以将ext类的文件系统信息全部显示出来，当然bmap是每个块组固定一个block的不用显示，imap比bmap更小所以也只占用1个block不用显示。 下图是一个文件系统的部分信息，在这些信息的后面还有每个块组的信息，其实这里面的很多信息都可以通过几个比较基本的元数据推导出来。 从这张表中能计算出文件系统的大小，该文件系统共4667136个blocks，每个block大小为4K，所以文件系统大小为4667136*4/1024/1024=17.8GB。 也能计算出分了多少个块组，因为每一个块组的block数量为32768，所以块组的数量为4667136/32768=142.4即143个块组。由于块组从0开始编号，所以最后一个块组编号为Group 142。如下图所示是最后一个块组的信息。 文件系统的完整结构将上文描述的bmap、inode table、imap、数据区的blocks和块组的概念组合起来就形成了一个文件系统，当然这还不是完整的文件系统。完整的文件系统如下图。 首先，该图中多了Boot Block、Super Block、GDT、Reserver GDT这几个概念。下面会分别介绍它们。 然后，图中指明了块组中每个部分占用的block数量，除了superblock、bmap、imap能确定占用1个block，其他的部分都不能确定占用几个block。 最后，图中指明了Superblock、GDT和Reserved GDT是同时出现且不一定存在于每一个块组中的，也指明了bmap、imap、inode table和data blocks是每个块组都有的。 引导块即上图中的Boot Block部分，也称为boot sector。它位于分区上的第一个块，占用1024字节，并非所有分区都有这个boot sector，只有装了操作系统的主分区和装了操作系统的逻辑分区才有。里面存放的也是boot loader，这段boot loader称为VBR(主分区装操作系统时)或EBR(扩展分区装操作系统时)，这里的Boot loader和mbr上的boot loader是存在交错关系的。开机启动的时候，首先加载mbr中的bootloader，然后定位到操作系统所在分区的boot serctor上加载此处的boot loader。如果是多系统，加载mbr中的bootloader后会列出操作系统菜单，菜单上的各操作系统指向它们所在分区的boot sector上。它们之间的关系如下图所示。 但是，这种方式的操作系统菜单早已经弃之不用了，而是使用grub来管理启动菜单。尽管如此，在安装操作系统时，仍然有一步是选择boot loader安装位置的步骤。 超级块(superblock)既然一个文件系统会分多个块组，那么文件系统怎么知道分了多少个块组呢？每个块组又有多少block多少inode号等等信息呢？还有，文件系统本身的属性信息如各种时间戳、block总数量和空闲数量、inode总数量和空闲数量、当前文件系统是否正常、什么时候需要自检等等，它们又存储在哪里呢？ 毫无疑问，这些信息必须要存储在block中。存储这些信息占用1024字节，所以也要一个block，这个block称为超级块(superblock)，它的block号可能为0也可能为1。如果block大小为1K，则引导块正好占用一个block，这个block号为0，所以superblock的号为1；如果block大小大于1K，则引导块和超级块同置在一个block中，这个block号为0。总之superblock的起止位置是第二个1024(1024-2047)字节。 使用df命令读取的就是每个文件系统的superblock，所以它的统计速度非常快。相反，用du命令查看一个较大目录的已用空间就非常慢，因为不可避免地要遍历整个目录的所有文件。 12345[root@xuexi ~]# df -hTFilesystem Type Size Used Avail Use% Mounted on/dev/sda3 ext4 18G 1.7G 15G 11% /tmpfs tmpfs 491M 0 491M 0% /dev/shm/dev/sda1 ext4 190M 32M 149M 18% /boot superblock对于文件系统而言是至关重要的，超级块丢失或损坏必将导致文件系统的损坏。所以旧式的文件系统将超级块备份到每一个块组中，但是这又有所空间浪费，所以ext2文件系统只在块组0、1和3、5、7幂次方的块组中保存超级块的信息，如Group9、Group25等。尽管保存了这么多的superblock，但是文件系统只使用第一个块组即Group0中超级块信息来获取文件系统属性，只有当Group0上的superblock损坏或丢失才会找下一个备份超级块复制到Group0中来恢复文件系统。 下图是一个ext4文件系统的superblock的信息，ext家族的文件系统都能使用dumpe2fs -h获取。 块组描述符表(GDT)既然文件系统划分了块组，那么每个块组的信息和属性元数据又保存在哪里呢？ ext文件系统每一个块组信息使用32字节描述，这32个字节称为块组描述符，所有块组的块组描述符组成块组描述符表GDT(group descriptor table)。 虽然每个块组都需要块组描述符来记录块组的信息和属性元数据，但是不是每个块组中都存放了块组描述符。ext文件系统的存储方式是：将它们组成一个GDT，并将该GDT存放于某些块组中，存放GDT的块组和存放superblock和备份superblock的块相同，也就是说它们是同时出现在某一个块组中的。读取时也总是读取Group0中的块组描述符表信息。 假如block大小为4KB的文件系统划分了143个块组，每个块组描述符32字节，那么GDT就需要143*32=4576字节即两个block来存放。这两个GDT block中记录了所有块组的块组信息，且存放GDT的块组中的GDT都是完全相同的。 下图是一个块组描述符的信息(通过dumpe2fs获取)。 保留GDT(Reserved GDT)保留GDT用于以后扩容文件系统使用，防止扩容后块组太多，使得块组描述符超出当前存储GDT的blocks。保留GDT和GDT总是同时出现，当然也就和superblock同时出现了。 例如前面143个块组使用了2个block来存放GDT，但是此时第二个block还空余很多空间，当扩容到一定程度时2个block已经无法再记录块组描述符了，这时就需要分配一个或多个Reserverd GDT的block来存放超出的块组描述符。 由于新增加了GDT block，所以应该让每一个保存GDT的块组都同时增加这一个GDT block，所以将保留GDT和GDT存放在同一个块组中可以直接将保留GDT变换为GDT而无需使用低效的复制手段备份到每个存放GDT的块组。 同理，新增加了GDT需要修改每个块组中superblock中的文件系统属性，所以将superblock和Reserverd GDT/GDT放在一起又能提升效率。 Data Block 如上图，除了Data Blocks其他的部分都解释过了。data block是直接存储数据的block，但事实上并非如此简单。 数据所占用的block由文件对应inode记录中的block指针找到，不同的文件类型，数据block中存储的内容是不一样的。以下是Linux中不同类型文件的存储方式。 对于常规文件，文件的数据正常存储在数据块中。 对于目录，该目录下的所有文件和一级子目录的目录名存储在数据块中。 文件名不是存储在其自身的inode中，而是存储在其所在目录的data block中。 对于符号链接，如果目标路径名较短则直接保存在inode中以便更快地查找，如果目标路径名较长则分配一个数据块来保存。 设备文件、FIFO和socket等特殊文件没有数据块，设备文件的主设备号和次设备号保存在inode中。 常规文件的存储就不解释了，下面分别解释特殊文件的存储方式。 目录文件的data block对于目录文件，其inode记录中存储的是目录的inode号、目录的属性元数据和目录文件的block指针，这里面没有存储目录自身文件名的信息。 而其data block的存储方式则如下图所示。 由图可知，在目录文件的数据块中存储了其下的文件名、目录名、目录本身的相对名称”.”和上级目录的相对名称”..”，还存储了指向inode table中这些文件名对应的inode号的指针(并非直接存储inode号码)、目录项长度rec_len、文件名长度name_len和文件类型file_type。注意到除了文件本身的inode记录了文件类型，其所在的目录的数据块也记录了文件类型。由于rec_len只能是4的倍数，所以需要使用”\\0”来填充name_len不够凑满4倍数的部分。至于rec_len具体是什么，只需知道它是一种偏移即可。 目录的data block中并没有直接存储目录中文件的inode号，它存储的是指向inode table中对应文件inode号的指针，暂且称之为inode指针(至此，已经知道了两种指针：一种是inode table中每个inode记录指向其对应data block的block指针，一个此处的inode指针)。一个很有说服力的例子，在目录只有读而没有执行权限的时候，使用”ls -l”是无法获取到其内文件inode号的，这就表明没有直接存储inode号。实际上，因为在创建文件系统的时候，inode号就已经全部划分好并在每个块组的inode table中存放好，inode table在块组中是有具体位置的，如果使用dumpe2fs查看文件系统，会发现每个块组的inode table占用的block数量是完全相同的，如下图是某分区上其中两个块组的信息，它们都占用249个block。 除了inode指针，目录的data block中还使用数字格式记录了文件类型，数字格式和文件类型的对应关系如下图。 注意到目录的data block中前两行存储的是目录本身的相对名称”.”和上级目录的相对名称”..”，它们实际上是目录本身的硬链接和上级目录的硬链接。硬链接的本质后面说明。 由此也就容易理解目录权限的特殊之处了。目录文件的读权限(r)和写权限(w)，都是针对目录文件的数据块本身。由于目录文件内只有文件名、文件类型和inode指针，所以如果只有读权限，只能获取文件名和文件类型信息，无法获取其他信息，尽管目录的data block中也记录着文件的inode指针，但定位指针是需要x权限的，因为其它信息都储存在文件自身对应的inode中，而要读取文件inode信息需要有目录文件的执行权限通过inode指针定位到文件对应的inode记录上。以下是没有目录x权限时的查询状态，可以看到除了文件名和文件类型，其余的全是”?”。 123456[lisi4@xuexi tmp]$ ll -i dls: cannot access d/hehe: Permission deniedls: cannot access d/haha: Permission deniedtotal 0? d????????? ? ? ? ? ? haha? -????????? ? ? ? ? ? hehe 注意，xfs文件系统和ext文件系统不一样，它连文件类型都无法获取。 符号链接存储方式符号链接即为软链接，类似于Windows操作系统中的快捷方式，它的作用是指向原文件或目录。 软链接之所以也被称为特殊文件的原因是：它一般情况下不占用data block，仅仅通过它对应的inode记录就能将其信息描述完成；符号链接的大小是其指向目标路径占用的字符个数，例如某个符号链接的指向方式为”rmt –&gt; ../sbin/rmt”，则其文件大小为11字节；只有当符号链接指向的目标的路径名较长(60个字节)时文件系统才会划分一个data block给它；它的权限如何也不重要，因它只是一个指向原文件的”工具”，最终决定是否能读写执行的权限由原文件决定，所以很可能ls -l查看到的符号链接权限为777。 注意，软链接的block指针存储的是目标文件名。也就是说，链接文件的一切都依赖于其目标文件名。这就解释了为什么/mnt的软链接/tmp/mnt在/mnt挂载文件系统后，通过软链接就能进入/mnt所挂载的文件系统。究其原因，还是因为其目标文件名”/mnt”并没有改变。 例如以下筛选出了/etc/下的符号链接，注意观察它们的权限和它们占用的空间大小。 1234567891011121314151617[root@xuexi ~]# ll /etc/ | grep &apos;^l&apos;lrwxrwxrwx. 1 root root 56 Feb 18 2016 favicon.png -&gt; /usr/share/icons/hicolor/16x16/apps/system-logo-icon.pnglrwxrwxrwx. 1 root root 22 Feb 18 2016 grub.conf -&gt; ../boot/grub/grub.conflrwxrwxrwx. 1 root root 11 Feb 18 2016 init.d -&gt; rc.d/init.dlrwxrwxrwx. 1 root root 7 Feb 18 2016 rc -&gt; rc.d/rclrwxrwxrwx. 1 root root 10 Feb 18 2016 rc0.d -&gt; rc.d/rc0.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc1.d -&gt; rc.d/rc1.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc2.d -&gt; rc.d/rc2.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc3.d -&gt; rc.d/rc3.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc4.d -&gt; rc.d/rc4.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc5.d -&gt; rc.d/rc5.dlrwxrwxrwx. 1 root root 10 Feb 18 2016 rc6.d -&gt; rc.d/rc6.dlrwxrwxrwx. 1 root root 13 Feb 18 2016 rc.local -&gt; rc.d/rc.locallrwxrwxrwx. 1 root root 15 Feb 18 2016 rc.sysinit -&gt; rc.d/rc.sysinitlrwxrwxrwx. 1 root root 14 Feb 18 2016 redhat-release -&gt; centos-releaselrwxrwxrwx. 1 root root 11 Apr 10 2016 rmt -&gt; ../sbin/rmtlrwxrwxrwx. 1 root root 14 Feb 18 2016 system-release -&gt; centos-release 设备文件、FIFO、套接字文件关于这3种文件类型的文件只需要通过inode就能完全保存它们的信息，它们不占用任何数据块，所以它们是特殊文件。 设备文件的主设备号和次设备号也保存在inode中。以下是/dev/下的部分设备信息。注意到它们的第5列和第6列信息，它们分别是主设备号和次设备号，主设备号标识每一种设备的类型，次设备号标识同种设备类型的不同编号；也注意到这些信息中没有大小的信息，因为设备文件不占用数据块所以没有大小的概念。 1234567891011[root@xuexi ~]# ll /dev | tailcrw-rw---- 1 vcsa tty 7, 129 Oct 7 21:26 vcsa1crw-rw---- 1 vcsa tty 7, 130 Oct 7 21:27 vcsa2crw-rw---- 1 vcsa tty 7, 131 Oct 7 21:27 vcsa3crw-rw---- 1 vcsa tty 7, 132 Oct 7 21:27 vcsa4crw-rw---- 1 vcsa tty 7, 133 Oct 7 21:27 vcsa5crw-rw---- 1 vcsa tty 7, 134 Oct 7 21:27 vcsa6crw-rw---- 1 root root 10, 63 Oct 7 21:26 vga_arbitercrw------- 1 root root 10, 57 Oct 7 21:26 vmcicrw-rw-rw- 1 root root 10, 56 Oct 7 21:27 vsockcrw-rw-rw- 1 root root 1, 5 Oct 7 21:26 zero inode基础知识每个文件都有一个inode，在将inode关联到文件后系统将通过inode号来识别文件，而不是文件名。并且访问文件时将先找到inode，通过inode中记录的block位置找到该文件。 硬链接虽然每个文件都有一个inode，但是存在一种可能：多个文件的inode相同，也就即inode号、元数据、block位置都相同，这是一种什么样的情况呢？能够想象这些inode相同的文件使用的都是同一条inode记录，所以代表的都是同一个文件，这些文件所在目录的data block中的inode指针目的地都是一样的，只不过各指针对应的文件名互不相同而已。这种inode相同的文件在Linux中被称为”硬链接”。 硬链接文件的inode都相同，每个文件都有一个”硬链接数”的属性，使用ls -l的第二列就是被硬链接数，它表示的就是该文件有几个硬链接。 1234567[root@xuexi ~]# ls -ltotal 48drwxr-xr-x 5 root root 4096 Oct 15 18:07 700-rw-------. 1 root root 1082 Feb 18 2016 anaconda-ks.cfg-rw-r--r-- 1 root root 399 Apr 29 2016 Identity.pub-rw-r--r--. 1 root root 21783 Feb 18 2016 install.log-rw-r--r--. 1 root root 6240 Feb 18 2016 install.log.syslog 例如下图描述的是dir1目录中的文件name1及其硬链接dir2/name2，右边分别是它们的inode和datablock。这里也看出了硬链接文件之间唯一不同的就是其所在目录中的记录不同。注意下图中有一列Link Count就是标记硬链接数的属性。 每创建一个文件的硬链接，实质上是多一个指向该inode记录的inode指针，并且硬链接数加1。 删除文件的实质是删除该文件所在目录data block中的对应的inode指针，所以也是减少硬链接次数，由于block指针是存储在inode中的，所以不是真的删除数据，如果仍有其他指针指向该inode，那么该文件的block指针仍然是可用的。当硬链接次数为1时再删除文件就是真的删除文件了，此时inode记录中block指针也将被删除。 不能跨分区创建硬链接，因为不同文件系统的inode号可能会相同，如果允许创建硬链接，复制到另一个分区时inode可能会和此分区已使用的inode号冲突。 硬链接只能对文件创建，无法对目录创建硬链接。之所以无法对目录创建硬链接，是因为文件系统已经把每个目录的硬链接创建好了，它们就是相对路径中的”.”和”..”，分别标识当前目录的硬链接和上级目录的硬链接。每一个目录中都会包含这两个硬链接，它包含了两个信息：(1)一个没有子目录的目录文件的硬链接数是2，其一是目录本身，即该目录datablock中的”.”，其二是其父目录datablock中该目录的记录，这两者都指向同一个inode号；(2)一个包含子目录的目录文件，其硬链接数是2+子目录数，因为每个子目录都关联一个父目录的硬链接”..”。很多人在计算目录的硬链接数时认为由于包含了”.”和”..”，所以空目录的硬链接数是2，这是错误的，因为”..”不是本目录的硬链接。另外，还有一个特殊的目录应该纳入考虑，即”/“目录，它自身是一个文件系统的入口，是自引用(下文中会解释自引用)的，所以”/“目录下的”.”和”..”的inode号相同，它自身不占用硬链接，因为其datablock中只记录inode号相同的”.”和”..”，不再像其他目录一样还记录一个名为”/“的目录，所以”/“的硬链接数也是2+子目录数，但这个2是”.”和”..”的结果。 12[root@xuexi ~]# ln /tmp /mydataln: `/tmp&apos;: hard link not allowed for directory 为什么文件系统自己创建好了目录的硬链接就不允许人为创建呢？从”.”和”..”的用法上考虑，如果当前目录为/usr，我们可以使用”./local”来表示/usr/local，但是如果我们人为创建了/usr目录的硬链接/tmp/husr，难道我们也要使用”/tmp/husr/local”来表示/usr/local吗？这其实已经是软链接的作用了。若要将其认为是硬链接的功能，这必将导致硬链接维护的混乱。 不过，通过mount工具的”–bind”选项，可以将一个目录挂载到另一个目录下，实现伪”硬链接”，它们的内容和inode号是完全相同的。 硬链接的创建方法： ln file_target link_name 。 软链接软链接就是字符链接，链接文件默认指的就是字符链接文件(注意不是字符设备)，使用”l”表示其类型。 软链接在功能上等价与Windows系统中的快捷方式，它指向原文件，原文件损坏或消失，软链接文件就损坏。可以认为软链接inode记录中的指针内容是目标路径的字符串。 创建方式： ln –s source_file softlink_name ，记住是source_file&lt;–link_name的指向关系(反箭头)，以前我老搞错位置。 查看软链接的值： readlink softlink_name 在设置软链接的时候，source_file虽然不要求是绝对路径，但建议给绝对路径。是否还记得软链接文件的大小？它是根据软链接所指向路径的字符数计算的，例如某个符号链接的指向方式为”rmt –&gt; ../sbin/rmt”，它的文件大小为11字节，也就是说只要建立了软链接后，软链接的指向路径是不会改变的，仍然是”../sbin/rmt”。如果此时移动软链接文件本身，它的指向是不会改变的，仍然是11个字符的”../sbin/rmt”，但此时该软链接父目录下可能根本就不存在/sbin/rmt，也就是说此时该软链接是一个被破坏的软链接。 inode深入inode大小和划分inode大小为128字节的倍数，最小为128字节。它有默认值大小，它的默认值由/etc/mke2fs.conf文件中指定。不同的文件系统默认值可能不同。 12345678910111213141516[root@xuexi ~]# cat /etc/mke2fs.conf[defaults] base_features = sparse_super,filetype,resize_inode,dir_index,ext_attr enable_periodic_fsck = 1 blocksize = 4096 inode_size = 256 inode_ratio = 16384[fs_types] ext3 = &#123; features = has_journal &#125; ext4 = &#123; features = has_journal,extent,huge_file,flex_bg,uninit_bg,dir_nlink,extra_isize inode_size = 256 &#125; 同样观察到这个文件中还记录了blocksize的默认值和inode分配比率inode_ratio。inode_ratio=16384表示每16384个字节即16KB就分配一个inode号，由于默认blocksize=4KB，所以每4个block就分配一个inode号。当然分配的这些inode号只是预分配，并不真的代表会全部使用，毕竟每个文件才会分配一个inode号。但是分配的inode自身会占用block，而且其自身大小256字节还不算小，所以inode号的浪费代表着空间的浪费。 既然知道了inode分配比率，就能计算出每个块组分配多少个inode号，也就能计算出inode table占用多少个block。 如果文件系统中大量存储电影等大文件，inode号就浪费很多，inode占用的空间也浪费很多。但是没办法，文件系统又不知道你这个文件系统是用来存什么样的数据，多大的数据，多少数据。 当然inodesize、inode分配比例、blocksize都可以在创建文件系统的时候人为指定。 ext文件系统预留的inode号Ext预留了一些inode做特殊特性使用，如下：某些可能并非总是准确，具体的inode号对应什么文件可以使用”find / -inum NUM”查看。 123456789101112Ext4的特殊inodeInode号 用途0 不存在0号inode1 虚拟文件系统，如/proc和/sys2 根目录3 ACL索引4 ACL数据5 Boot loader6 未删除的目录7 预留的块组描述符inode8 日志inode11 第一个非预留的inode，通常是lost+found目录 所以在ext4文件系统的dumpe2fs信息中，能观察到fisrt inode号可能为11也可能为12。 并且注意到”/“的inode号为2，这个特性在文件访问时会用上。 需要注意的是，每个文件系统都会分配自己的inode号，不同文件系统之间是可能会出现使用相同inode号文件的。例如： 123456[root@xuexi ~]# find / -ignore_readdir_race -inum 2 -ls 2 4 dr-xr-xr-x 22 root root 4096 Jun 9 09:56 / 2 2 dr-xr-xr-x 5 root root 1024 Feb 25 11:53 /boot 2 0 c--------- 1 root root Jun 7 02:13 /dev/pts/ptmx 2 0 -rw-r--r-- 1 root root 0 Jun 6 18:13 /proc/sys/fs/binfmt_misc/status 2 0 drwxr-xr-x 3 root root 0 Jun 6 18:13 /sys/fs 从结果中可见，除了根的Inode号为2，还有几个文件的inode号也是 2，它们都属于独立的文件系统，有些是虚拟文件系统，如/proc和/sys。 ext2/3的inode直接、间接寻址前文说过，inode中保存了blocks指针，但是一条inode记录中能保存的指针数量是有限的，否则就会超出inode大小(128字节或256字节)。 在ext2和ext3文件系统中，一个inode中最多只能有15个指针，每个指针使用i_block[n]表示。 前12个指针i_block[0]到i_block[11]是直接寻址指针，每个指针指向一个数据区的block。如下图所示。 第13个指针i_block[12]是一级间接寻址指针，它指向一个仍然存储了指针的block即i_block[12] –&gt; Pointerblock –&gt; datablock。 第14个指针i_block[13]是二级间接寻址指针，它指向一个仍然存储了指针的block，但是这个block中的指针还继续指向其他存储指针的block，即i_block[13] –&gt; Pointerblock1 –&gt; PointerBlock2 –&gt; datablock。 第15个指针i_block[14]是三级间接寻址指针，它指向一个任然存储了指针的block，这个指针block下还有两次指针指向。即i_block[13] –&gt; Pointerblock1 –&gt; PointerBlock2 –&gt; PointerBlock3 –&gt; datablock。 其中由于每个指针大小为4字节，所以每个指针block能存放的指针数量为BlockSize/4byte。例如blocksize为4KB，那么一个Block可以存放4096/4=1024个指针。 如下图。 为什么要分间接和直接指针呢？如果一个inode中15个指针全是直接指针，假如每个block的大小为1KB，那么15个指针只能指向15个block即15KB的大小，由于每个文件对应一个inode号，所以就限制了每个文件最大为15*1=15KB，这显然是不合理的。 如果存储大于15KB的文件而又不太大的时候，就占用一级间接指针i_block[12]，这时可以存放指针数量为1024/4+12=268，所以能存放268KB的文件。 如果存储大于268K 的文件而又不太大的时候，就继续占用二级指针i_block[13]，这时可以存放指针数量为[1024/4]^2+1024/4+12=65804，所以能存放65804KB=64M左右的文件。 如果存放的文件大于64M，那么就继续使用三级间接指针i_block[14]，存放的指针数量为[1024/4]^3+[1024/4]^2+[1024/4]+12=16843020个指针，所以能存放16843020KB=16GB左右的文件。 如果blocksize=4KB呢？那么最大能存放的文件大小为([4096/4]^3+[4096/4]^2+[4096/4]+12)*4/1024/1024/1024=4T左右。当然这样计算出来的不一定就是最大能存放的文件大小，它还受到另一个条件的限制。这里的计算只是表明一个大文件是如何寻址和分配的。 其实看到这里的计算数值，就知道ext2和ext3对超大文件的存取效率是低下的，它要核对太多的指针，特别是4KB大小的blocksize时。而ext4针对这一点就进行了优化，ext4使用extent的管理方式取代ext2和ext3的块映射，大大提高了效率也降低了碎片。 单文件系统中文件操作的原理在Linux上执行删除、复制、重命名、移动等操作时，它们是怎么进行的呢？还有访问文件时是如何找到它的呢？其实只要理解了前文中介绍的几个术语以及它们的作用就很容易知道文件操作的原理了。 注：在这一小节所解释的都是在单个文件系统下的行为，在多个文件系统中如何请看下一个小节：多文件系统关联。 读取文件当执行”cat /var/log/messages”命令在系统内部进行了什么样的步骤呢？该命令能被成功执行涉及了cat命令的寻找、权限判断以及messages文件的寻找和权限判断等等复杂的过程。这里只解释和本节内容相关的如何寻找到被cat的/var/log/messages文件。 找到根文件系统的块组描述符表所在的blocks，读取GDT(已在内存中)找到inode table的block号。 因为GDT总是和superblock在同一个块组，而superblock总是在分区的第1024-2047个字节，所以很容易就知道第一个GDT所在的块组以及GDT在这个块组中占用了哪些block。其实GDT早已经在内存中了，在系统开机的时候会挂在根文件系统，挂载的时候就已经将所有的GDT放进内存中。 在inode table的block中定位到根”/“的inode，找出”/“指向的data block。 前文说过，ext文件系统预留了一些inode号，其中”/“的inode号为2，所以可以根据inode号直接定位根目录文件的data block。 在”/“的datablock中记录了var目录名和指向var目录文件inode的指针，并找到该inode记录，inode记录中存储了指向var的block指针，所以也就找到了var目录文件的data block。 通过var目录的inode指针，可以寻找到var目录的inode记录，但是指针定位的过程中，还需要知道该inode记录所在的块组以及所在的inode table，所以需要读取GDT，同样，GDT已经缓存到了内存中。 在var的data block中记录了log目录名和其inode指针，通过该指针定位到该inode所在的块组及所在的inode table，并根据该inode记录找到log的data block。 在log目录文件的data block中记录了messages文件名和对应的inode指针，通过该指针定位到该inode所在的块组及所在的inode table，并根据该inode记录找到messages的data block。 最后读取messages对应的datablock。将上述步骤中GDT部分的步骤简化后比较容易理解。如下:找到GDT–&gt;找到”/“的inode–&gt;找到/的数据块读取var的inode–&gt;找到var的数据块读取log的inode–&gt;找到log的数据块读取messages的inode–&gt;找到messages的数据块并读取它们。 删除、重命名和移动文件注意这里是不跨越文件系统的操作行为。 删除文件分为普通文件和目录文件，知道了这两种类型的文件的删除原理，就知道了其他类型特殊文件的删除方法。 对于删除普通文件：(1)找到文件的inode和data block(根据前一个小节中的方法寻找)；(2)将inode table中该inode记录中的data block指针删除；(3)在imap中将该文件的inode号标记为未使用；(4)在其所在目录的data block中将该文件名所在的记录行删除，删除了记录就丢失了指向inode的指针；(5)将bmap中data block对应的block号标记为未使用。 对于删除目录文件：找到目录和目录下所有文件、子目录、子文件的inode和data block；在imap中将这些inode号标记为未使用；将bmap中将这些文件占用的 block号标记为未使用；在该目录的父目录的data block中将该目录名所在的记录行删除。需要注意的是，删除父目录data block中的记录是最后一步，如果该步骤提前，将报目录非空的错误，因为在该目录中还有文件占用。 关于上面的(2)-(5)：当(2)中删除data block指针后，将无法再找到这个文件的数据；当(3)标记inode号未使用，表示该inode号可以被后续的文件重用；当(4)删除目录data block中关于该文件的记录，真正的删除文件，外界再也定位也无法看到这个文件了；当(5)标记data block为未使用后，表示开始释放空间，这些data block可以被其他文件重用。 注意，在第(5)步之前，由于data block还未被标记为未使用，在superblock中仍然认为这些data block是正在使用中的。这表示尽管文件已经被删除了，但空间却还没有释放，df也会将其统计到已用空间中(df是读取superblock中的数据块数量，并计算转换为空间大小)。 什么时候会发生这种情况呢？当一个进程正在引用文件时将该文件删除，就会出现文件已删除但空间未释放的情况。这时步骤已经进行到(4)，外界无法再找到该文件，但由于进程在加载该文件时已经获取到了该文件所有的data block指针，该进程可以获取到该文件的所有数据，但却暂时不会释放该文件空间。直到该进程结束，文件系统才将未执行的步骤(5)继续完成。这也是为什么有时候du的统计结果比df小的原因，关于du和df统计结果的差别，详细内容见：详细分析du和df的统计结果为什么不一样。 重命名文件分为同目录内重命名和非同目录内重命名。非同目录内重命名实际上是移动文件的过程，见下文。 同目录内重命名文件的动作仅仅只是修改所在目录data block中该文件记录的文件名部分，不是删除再重建的过程。 如果重命名时有文件名冲突(该目录内已经存在该文件名)，则提示是否覆盖。覆盖的过程是覆盖目录data block中冲突文件的记录。例如/tmp/下有a.txt和a.log，若将a.txt重命名为a.log，则提示覆盖，若选择覆盖，则/tmp的data block中关于a.log的记录被覆盖，此时它的指针是指向a.txt的inode。 移动文件同文件系统下移动文件实际上是修改目标文件所在目录的data block，向其中添加一行指向inode table中待移动文件的inode指针，如果目标路径下有同名文件，则会提示是否覆盖，实际上是覆盖目录data block中冲突文件的记录，由于同名文件的inode记录指针被覆盖，所以无法再找到该文件的data block，也就是说该文件被标记为删除(如果多个硬链接数，则另当别论)。 所以在同文件系统内移动文件相当快，仅仅在所在目录data block中添加或覆盖了一条记录而已。也因此，移动文件时，文件的inode号是不会改变的。 对于不同文件系统内的移动，相当于先复制再删除的动作。见后文。 关于文件移动，在Linux环境下有一个非常经典网上却又没任何解释的问题：/tmp/a/a能覆盖为/tmp/a吗？答案是不能，但windows能。为什么不能？见mv的一个经典问题(mv的本质)。 存储和复制文件对于文件存储 (1).读取GDT，找到各个(或部分)块组imap中未使用的inode号，并为待存储文件分配inode号； (2).在inode table中完善该inode号所在行的记录； (3).在目录的data block中添加一条该文件的相关记录； (4).将数据填充到data block中。注意，填充到data block中的时候会调用block分配器：一次分配4KB大小的block数量，当填充完4KB的data block后会继续调用block分配器分配4KB的block，然后循环直到填充完所有数据。也就是说，如果存储一个100M的文件需要调用block分配器100*1024/4=25600次。 另一方面，在block分配器分配block时，block分配器并不知道真正有多少block要分配，只是每次需要分配时就分配，在每存储一个data block前，就去bmap中标记一次该block已使用，它无法实现一次标记多个bmap位。这一点在ext4中进行了优化。 (5)填充完之后，去inode table中更新该文件inode记录中指向data block的寻址指针。 对于复制，完全就是另一种方式的存储文件。步骤和存储文件的步骤一样。 多文件系统关联在单个文件系统中的文件操作和多文件系统中的操作有所不同。本文将对此做出非常详细的说明。 根文件系统的特殊性这里要明确的是，任何一个文件系统要在Linux上能正常使用，必须挂载在某个已经挂载好的文件系统中的某个目录下，例如/dev/cdrom挂载在/mnt上，/mnt目录本身是在”/“文件系统下的。而且任意文件系统的一级挂载点必须是在根文件系统的某个目录下，因为只有”/“是自引用的。这里要说明挂载点的级别和自引用的概念。 假如/dev/sdb1挂载在/mydata上，/dev/cdrom挂载在/mydata/cdrom上，那么/mydata就是一级挂载点，此时/mydata已经是文件系统/dev/sdb1的入口了，而/dev/cdrom所挂载的目录/mydata/cdrom是文件系统/dev/sdb1中的某个目录，那么/mydata/cdrom就是二级挂载点。一级挂载点必须在根文件系统下，所以可简述为：文件系统2挂载在文件系统1中的某个目录下，而文件系统1又挂载在根文件系统中的某个目录下。 再解释自引用。首先要说的是，自引用的只能是文件系统，而文件系统表现形式是一个目录，所以自引用是指该目录的data block中，”.”和”..”的记录中的inode指针都指向inode table中同一个inode记录，所以它们inode号是相同的，即互为硬链接。而根文件系统是唯一可以自引用的文件系统。 1234[root@xuexi /]# ll -ai /total 102 2 dr-xr-xr-x. 22 root root 4096 Jun 6 18:13 . 2 dr-xr-xr-x. 22 root root 4096 Jun 6 18:13 .. 由此也能解释cd /.和cd /..的结果都还是在根下，这是自引用最直接的表现形式。 1234[root@xuexi tmp]# cd /.[root@xuexi /]#[root@xuexi tmp]# cd /..[root@xuexi /]# 注意，根目录下的”.”和”..”都是”/“目录的硬链接，且其datablock中不记录名为”/“的条目，因此除去根目录下子目录数后的硬链接数为2。 1234[root@server2 tmp]# a=$(ls -ld / | awk &apos;&#123;print $2&#125;&apos;)[root@server2 tmp]# b=$(ls -l / | grep &quot;^d&quot; |wc -l)[root@server2 tmp]# echo $((a - b))2 挂载文件系统的细节挂载文件系统到某个目录下，例如”mount /dev/cdrom /mnt”，挂载成功后/mnt目录中的文件全都暂时不可见了，且挂载后权限和所有者(如果指定允许普通用户挂载)等的都改变了，知道为什么吗？ 下面就以通过”mount /dev/cdrom /mnt”为例，详细说明挂载过程中涉及的细节。 在将文件系统/dev/cdrom(此处暂且认为它是文件系统)挂载到挂载点/mnt之前，挂载点/mnt是根文件系统中的一个目录，”/“的data block中记录了/mnt的一些信息，其中包括inode指针inode_n，而在inode table中，/mnt对应的inode记录中又存储了block指针block_n，此时这两个指针还是普通的指针。 当文件系统/dev/cdrom挂载到/mnt上后，/mnt此时就已经成为另一个文件系统的入口了，因此它需要连接两边文件系统的inode和data block。但是如何连接呢？如下图。 在根文件系统的inode table中，为/mnt重新分配一个inode记录m，该记录的block指针block_m指向文件系统/dev/cdrom中的data block。既然为/mnt分配了新的inode记录m，那么在”/“目录的data block中，也需要修改其inode指针为inode_m以指向m记录。同时，原来inode table中的inode记录n就被标记为暂时不可用。 block_m指向的是文件系统/dev/cdrom的data block，所以严格说起来，除了/mnt的元数据信息即inode记录m还在根文件系统上，/mnt的data block已经是在/dev/cdrom中的了。这就是挂载新文件系统后实现的跨文件系统，它将挂载点的元数据信息和数据信息分别存储在不同的文件系统上。 挂载完成后，将在/proc/self/{mounts,mountstats,mountinfo}这三个文件中写入挂载记录和相关的挂载信息，并会将/proc/self/mounts中的信息同步到/etc/mtab文件中，当然，如果挂载时加了-n参数，将不会同步到/etc/mtab。 而卸载文件系统，其实质是移除临时新建的inode记录(当然，在移除前会检查是否正在使用)及其指针，并将指针指回原来的inode记录，这样inode记录中的block指针也就同时生效而找回对应的data block了。由于卸载只是移除inode记录，所以使用挂载点和文件系统都可以实现卸载，因为它们是联系在一起的。 下面是分析或结论。 (1).挂载点挂载时的inode记录是新分配的。 挂载前挂载点/mnt的inode号1234[root@server2 tmp]# ll -id /mnt100663447 drwxr-xr-x. 2 root root 6 Aug 12 2015 /mnt[root@server2 tmp]# mount /dev/cdrom /mnt 挂载后挂载点的inode号12[root@server2 tmp]# ll -id /mnt 1856 dr-xr-xr-x 8 root root 2048 Dec 10 2015 mnt 由此可以验证，inode号确实是重新分配的。 (2).挂载后，挂载点的内容将暂时不可见、不可用，卸载后文件又再次可见、可用。 123# 在挂载前，向挂载点中创建几个文件[root@server2 tmp]# touch /mnt/a.txt[root@server2 tmp]# mkdir /mnt/abcdir 12345678910111213141516171819202122232425# 挂载[root@server2 tmp]# mount /dev/cdrom /mnt# 挂载后，挂载点中将找不到刚创建的文件[root@server2 tmp]# ll /mnttotal 636-r--r--r-- 1 root root 14 Dec 10 2015 CentOS_BuildTagdr-xr-xr-x 3 root root 2048 Dec 10 2015 EFI-r--r--r-- 1 root root 215 Dec 10 2015 EULA-r--r--r-- 1 root root 18009 Dec 10 2015 GPLdr-xr-xr-x 3 root root 2048 Dec 10 2015 imagesdr-xr-xr-x 2 root root 2048 Dec 10 2015 isolinuxdr-xr-xr-x 2 root root 2048 Dec 10 2015 LiveOSdr-xr-xr-x 2 root root 612352 Dec 10 2015 Packagesdr-xr-xr-x 2 root root 4096 Dec 10 2015 repodata-r--r--r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-7-r--r--r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7-r--r--r-- 1 root root 2883 Dec 10 2015 TRANS.TBL# 卸载后，挂载点/mnt中的文件将再次可见[root@server2 tmp]# umount /mnt[root@server2 tmp]# ll /mnttotal 0drwxr-xr-x 2 root root 6 Jun 9 08:18 abcdir-rw-r--r-- 1 root root 0 Jun 9 08:18 a.txt 之所以会这样，是因为挂载文件系统后，挂载点原来的inode记录暂时被标记为不可用，关键是没有指向该inode记录的inode指针了。在卸载文件系统后，又重新启用挂载点原来的inode记录，”/“目录下的mnt的inode指针又重新指向该inode记录。 (3).挂载后，挂载点的元数据和data block是分别存放在不同文件系统上的。 (4).挂载点即使在挂载后，也还是属于源文件系统的文件。 多文件系统操作关联假如下图中的圆代表一块硬盘，其中划分了3个区即3个文件系统。其中根是根文件系统，/mnt是另一个文件系统A的入口，A文件系统挂载在/mnt上，/mnt/cdrom也是一个文件系统B的入口，B文件系统挂载在/mnt/cdrom上。每个文件系统都维护了一些inode table，这里假设图中的inode table是每个文件系统所有块组中的inode table的集合表。 如何读取/var/log/messages呢？这是和”/“在同一个文件系统的文件读取，在前面单文件系统中已经详细说明了。 但如何读取A文件系统中的/mnt/a.log呢？首先，从根文件系统找到/mnt的inode记录，这是单文件系统内的查找；然后根据此inode记录的block指针，定位到/mnt的data block中，这些block是A文件系统的data block；然后从/mnt的data block中读取a.log记录，并根据a.log的inode指针定位到A文件系统的inode table中对应a.log的inode记录；最后从此inode记录的block指针找到a.log的data block。至此，就能读取到/mnt/a.log文件的内容。 下图能更完整的描述上述过程。 那么又如何读取/mnt/cdrom中的/mnt/cdrom/a.rpm呢？这里cdrom代表的文件系统B挂载点位于/mnt下，所以又多了一个步骤。先找到”/“，再找到根中的mnt，进入到mnt文件系统中，找到cdrom的data block，再进入到cdrom找到a.rpm。也就是说，mnt目录文件存放位置是根，cdrom目录文件存放位置是mnt，最后a.rpm存放的位置才是cdrom。 继续完善上图。如下。 ext3文件系统的日志功能相比ext2文件系统，ext3多了一个日志功能。 在ext2文件系统中，只有两个区：数据区和元数据区。如果正在向data block中填充数据时突然断电，那么下一次启动时就会检查文件系统中数据和状态的一致性，这段检查和修复可能会消耗大量时间，甚至检查后无法修复。之所以会这样是因为文件系统在突然断电后，它不知道上次正在存储的文件的block从哪里开始、哪里结束，所以它会扫描整个文件系统进行排除(也许是这样检查的吧)。 而在创建ext3文件系统时会划分三个区：数据区、日志区和元数据区。每次存储数据时，先在日志区中进行ext2中元数据区的活动，直到文件存储完成后标记上commit才将日志区中的数据转存到元数据区。当存储文件时突然断电，下一次检查修复文件系统时，只需要检查日志区的记录，将bmap对应的data block标记为未使用，并把inode号标记未使用，这样就不需要扫描整个文件系统而耗费大量时间。 虽说ext3相比ext2多了一个日志区转写元数据区的动作而导致ext3相比ext2性能要差一点，特别是写众多小文件时。但是由于ext3其他方面的优化使得ext3和ext2性能几乎没有差距。 ext4文件系统回顾前面关于ext2和ext3文件系统的存储格式，它使用block为存储单元，每个block使用bmap中的位来标记是否空闲，尽管使用划分块组的方法优化提高了效率，但是一个块组内部仍然使用bmap来标记该块组内的block。对于一个巨大的文件，扫描整个bmap都将是一件浩大的工程。另外在inode寻址方面，ext2/3使用直接和间接的寻址方式，对于三级间接指针，可能要遍历的指针数量是非常非常巨大的。 ext4文件系统的最大特点是在ext3的基础上使用区(extent，或称为段)的概念来管理。一个extent尽可能的包含物理上连续的一堆block。inode寻址方面也一样使用区段树的方式进行了改进。默认情况下，EXT4不再使用EXT3的block mapping分配方式 ，而改为Extent方式分配。 (1). 关于EXT4的结构特征 EXT4在总体结构上与EXT3相似，大的分配方向都是基于相同大小的块组，每个块组内分配固定数量的inode、可能的superblock(或备份)及GDT。 EXT4的inode 结构做了重大改变，为增加新的信息，大小由EXT3的128字节增加到默认的256字节，同时inode寻址索引不再使用EXT3的”12个直接寻址块+1个一级间接寻址块+1个二级间接寻址块+1个三级间接寻址块”的索引模式，而改为4个Extent片断流，每个片断流设定片断的起始block号及连续的block数量(有可能直接指向数据区，也有可能指向索引块区)。 片段流即下图中索引节点(inde node block)部分的绿色区域，每个15字节，共60字节。 (2). EXT4删除数据的结构更改。 EXT4删除数据后，会依次释放文件系统bitmap空间位、更新目录结构、释放inode空间位。 (3). ext4使用多block分配方式。 在存储数据时，ext3中的block分配器一次只能分配4KB大小的Block数量，而且每存储一个block前就标记一次bmap。假如存储1G的文件，blocksize是4KB，那么每存储完一个Block就将调用一次block分配器，即调用的次数为10241024/4KB=262144次，标记bmap的次数也为10241024/4=262144次。 而在ext4中根据区段来分配，可以实现调用一次block分配器就分配一堆连续的block，并在存储这一堆block前一次性标记对应的bmap。这对于大文件来说极大的提升了存储效率。 ext类的文件系统的缺点最大的缺点是它在创建文件系统的时候就划分好一切需要划分的东西，以后用到的时候可以直接进行分配，也就是说它不支持动态划分和动态分配。对于较小的分区来说速度还好，但是对于一个超大的磁盘，速度是极慢极慢的。例如将一个几十T的磁盘阵列格式化为ext4文件系统，可能你会因此而失去一切耐心。 除了格式化速度超慢以外，ext4文件系统还是非常可取的。当然，不同公司开发的文件系统都各有特色，最主要的还是根据需求选择合适的文件系统类型。 虚拟文件系统VFS每一个分区格式化后都可以建立一个文件系统，Linux上可以识别很多种文件系统，那么它是如何识别的呢？另外，在我们操作分区中的文件时，并没有指定过它是哪个文件系统的，各种不同的文件系统如何被我们用户以无差别的方式操作呢？这就是虚拟文件系统的作用。 虚拟文件系统为用户操作各种文件系统提供了通用接口，使得用户执行程序时不需要考虑文件是在哪种类型的文件系统上，应该使用什么样的系统调用来操作该文件。有了虚拟文件系统，只要将所有需要执行的程序调用VFS的系统调用就可以了，剩下的动作由VFS来帮忙完成。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"文件系统","slug":"文件系统","permalink":"http://yoursite.com/tags/文件系统/"}]},{"title":"DHCP服务","slug":"DHCP 服务","date":"2017-04-09T16:00:00.000Z","updated":"2018-06-15T14:41:09.146Z","comments":true,"path":"2017/04/10/DHCP 服务/","link":"","permalink":"http://yoursite.com/2017/04/10/DHCP 服务/","excerpt":"DHCP前身是BOOTP，在Linux的网卡配置中也能看到显示的是BOOTP，DHCP引进一个bootp没有的概念：租约。bootp分配的地址是永久的，而dhcp分配的地址是可以有期限的 。12[root@xuexi vsftpd]# grep -i bootproto /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=dhcp DHCP可以自动分配IP、子网掩码、网关、DNS。DHCP客户端使用的端口68，服务端使用端口67，使用的UDP应用层的协议。","text":"DHCP前身是BOOTP，在Linux的网卡配置中也能看到显示的是BOOTP，DHCP引进一个bootp没有的概念：租约。bootp分配的地址是永久的，而dhcp分配的地址是可以有期限的 。12[root@xuexi vsftpd]# grep -i bootproto /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=dhcp DHCP可以自动分配IP、子网掩码、网关、DNS。DHCP客户端使用的端口68，服务端使用端口67，使用的UDP应用层的协议。DHCP一般不为服务器分配IP，因为他们要使用固定IP，所以DHCP一般只为办公环境的主机分配IP。 DHCP服务器和客户端需要在一个局域网内，在为客户端分配IP的时候需要进行多次广播。但DHCP也可以为其他网段内主机分配IP，只要连接两个网段中间的路由器能转发DHCP配置请求即可，但这要求路由器配置中继功能。 DHCP客户端请求过程（4步请求过程）1）搜索阶段：客户端广播方式发送报文，搜索DHCP服务器。此时网段内所有机器都收到报文，只有DHCP服务器返回消息。2）提供阶段：众多DHCP服务器返回报文信息，并从地址池找一个IP提供给客户端。因为此时客户端还没有IP，所以返回信息也是以广播的方式返回的。3）选择阶段：选择一个DHCP服务器，使用它提供的IP。然后发送广播包，告诉众多DHCP服务器，其已经选好DHCP服务器以及IP地址。此后没有入选的DHCP就可以将原本想分配的IP分配给其他主机. 客户端选择第一个接收到的IP。谁的IP先到客户端的速度是不可控的。但是如果在配置文件里开启了authoritative选项则表示该服务器是权威服务器，其他DHCP服务器将失效，如果多台服务器都配置了这个权威选项，则还是竞争机制；通过MAC地址给客户端配置固定IP也会优先于普通的动态DHCP分配。另外Windows的DHCP服务端回应Windows客户端比Linux更快。4）确认阶段：DHCP服务器收到回应，向客户端发送一个包含IP的数据包，确认租约，并指定租约时长。如果DHCP服务器要跨网段提供服务，一样是四步请求，只不过是每一步中间都多了一个路由器和DHCP服务器之间的单播通信。 1） 客户端广播方式发送报文，搜索DHCP服务器。所有机器包括路由器都收到报文，路由器配置了中继，知道搜索消息后单播给DHCP服务器；2）DHCP服务器单播返回信息给路由器，路由器再广播给客户端；3）客户端选择DHCP服务器提供的IP，并广播信息告诉它我选好了，路由器单播给DHCP服务器；4）DHCP服务器收到信息将确认信息单播给路由器，路由器单播给客户端。 所以DHCP的4步请求： 12Client--&gt; DHCPDISCOVER # 广播：客户端发现DHCP服务器 DHCPOFFER &lt;-- Server # 广播：服务端提供IP给客户端 12client--&gt; DCHPREQUEST # 广播：客户端请求使用提供的IP DCHPACK &lt;-- Server # 单播：服务端进行确认，订立租约等信息 续租的过程：12client--&gt; DHCPREQUEST # 单播：继续请求使用提供的IP DHCPACK &lt;-- Server # 单播：确认续租 HCP服务器不跨网段提供服务时，它自己的IP地址必须要和地址池中全部IP在同一网络中。DHCP服务器跨网段提供服务时，它自己的IP地址必须要和地址池中的一部分IP在同一网络中，另一部分提供给其他网段。因为如果自己的IP完全不在自己的网络中而只提供其他网段的IP，更好的做法是将DHCP服务器设在那个需要DHCP服务的网络中。当计算机从一个子网移到另一个子网，找的DHCP服务器不同，因为旧的租约还存在，会先续租，新的DHCP服务器肯定拒绝它的续租请求，这时将重新开始四步请求。有些机器希望一直使用一个固定的IP，也就是静态IP，除了手动进行配置，DHCP服务器也可以实现这个功能。DHCP服务器可以根据MAC地址来分配这台机器固定IP地址（保留地址），即使重启或重装了系统也不会改变根据MAC地址分配的地址。假如在一个正常联网有DHCP服务器的网段内因为做实验练习的缘故新建立了一台DHCP服务器，但是这台DHCP服务器不能上网，会导致什么后果？使用DHCP分配地址的客户端至少会有续租的请求，如果没有续租成功，或者有新的计算机加入这个网络，那么进行四步请求，有可能会请求到这个不能连网的DHCP服务器上，那么他也就不能上网了。特别是Windows的DHCP服务端回应Windows客户端速度比Linux回应快。安装和配置DHCP服务123456789[root@xuexi ~]# yum -y install dhcp[root@xuexi ~]# rpm -ql dhcp/etc/dhcp/dhcpd.conf # DHCP配置文件/etc/sysconfig/dhcpd/usr/sbin/dhcpd # DHCP服务程序/usr/sbin/dhcrelay # 中继命令程序，用于跨网段提供DHCP服务/var/lib/dhcpd/dhcpd.leases # 存放租借信息（如IP）和租约信息（如租约时长）/usr/share/doc/dhcp-4.1.1/dhcpd.conf.sample # 配置文件的范例文件 123可以将dhcpd.conf.sample复制到/etc/。 [root@xuexi ~]# cp /usr/share/dhcp-4.1.1/dhcpd.conf.sample /etc/dhcpd.conf 以下是dhcpd.conf中部分配置项。123456789101112131415161718192021# 每行分号结束ddns-update-style none; # 动态dns相关，几乎不开启它。也就是不管它。ignore client-updates; # 和上面的相关，也不管它authoritative # 声明为权威服务器next-server marvin.redhat.com; # PXE环境下指定的提供引导程序的文件服务器# DHCP配置文件里必须配置一个地址池，其和DHCP服务器自身IP在同一网段subnet 10.5.5.0 netmask 255.255.255.224 &#123; range 10.5.5.26 10.5.5.30; # 地址池 option domain-name-servers ns1.internal.example.org; # 为客户端指明DNS服务器地址，可以是多个，最多三个 option domain-name &quot;internal.example.org&quot;; # 为客户端指明DNS名字，定义了它会覆盖客户端/etc/resolv.conf里的配置 option routers 10.5.5.1; # 默认路由，其实就是网关 option broadcast-address 10.5.5.31; # 广播地址，不设置时默认会根据A/B/C类地址自动计算 default-lease-time 600; # 默认租约时长 max-lease-time 7200; # 最大租约时长&#125;#下面的是绑定MAC地址设置保留地址，保留地址不能是地址池中的地址host fantasia &#123; # 固定地址的配置，host后面的是标识符，没意义hardware ethernet 08:00:07:26:c0:a5; fixed-address 192.168.100.3; # 根据MAC地址分配的固定IP &#125; 如果不让dhcp修改/etc/resolv.conf里的内容，就在网卡配置文件/etc/sysconfig/network-scripts/ifcfg-ethX里添加一行选项：PEERDNS=no。在客户端如何获取动态分配的地址呢？方法一：service network restart但是每次重启网络很麻烦，可以使用客户端命令dhclient。方法二：直接执行dhclient命令这种方法下会显示4部请求中需要显示的步骤信息，以及最终分配的地址，所以是一个很好的理解dhcp工作的工具。但是这种方法只能使用一次，第二次执行命令会提示该进程已经在执行，因为dhclient是一个进程。可以kill掉该进程再执行dhclient，或者使用dhclient -d选项。方法三：dhclient -d如何重新获取IP地址每次重启网卡默认都获取的同一个ip，有时候想换个ip都很麻烦。在/var/lib/dhclient/目录下有”.leases”文件，将它们清空或者删除这些文件中对应网卡的部分，再重启网络就可以获取新的动态ip。12345678910111213141516[root@xuexi ~]# cat /var/lib/dhclient/dhclient-eth0.leases lease &#123; interface &quot;eth0&quot;; fixed-address 192.168.100.16; option subnet-mask 255.255.255.0; option routers 192.168.100.2; option dhcp-lease-time 1800; option dhcp-message-type 5; option domain-name-servers 192.168.100.2; option dhcp-server-identifier 192.168.100.254; option broadcast-address 192.168.100.255; option domain-name &quot;localdomain&quot;; renew 3 2017/02/15 12:28:27; rebind 3 2017/02/15 12:42:39; expire 3 2017/02/15 12:46:24;&#125; 或者，在/etc/sysconfig/network-scripts/ifcfg-eth0加入”DHCPRELEASE=yes”。当运行ifdown eth0的时候就会发出dhcprelase报文，查看/etc/sysconfig/network-scripts/ifdown-eth脚本中实际上是调用dhclient命令，用下面这个命令应该也可以。1/sbin/dhclient -r eth0","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"DHCP","slug":"DHCP","permalink":"http://yoursite.com/tags/DHCP/"}]},{"title":"DNS 服务从基础到深入","slug":"DNS 服务从基础到深入","date":"2017-02-14T16:00:00.000Z","updated":"2018-06-15T12:22:30.217Z","comments":true,"path":"2017/02/15/DNS 服务从基础到深入/","link":"","permalink":"http://yoursite.com/2017/02/15/DNS 服务从基础到深入/","excerpt":"一 简介 DNS是Domain name system的简称，有些地方也称为Domain name server，这东西是一个很大的话题。如果不是要配置DNS服务，只需要理解DNS的解析流程和DNS有关的基本知识即可。如果要配置DNS服务，则可以看完全文。推荐阅读书籍：《DNS &amp; bind》，第四版有中文版，第五版目前只有英文版。 DNS必懂基础","text":"一 简介 DNS是Domain name system的简称，有些地方也称为Domain name server，这东西是一个很大的话题。如果不是要配置DNS服务，只需要理解DNS的解析流程和DNS有关的基本知识即可。如果要配置DNS服务，则可以看完全文。推荐阅读书籍：《DNS &amp; bind》，第四版有中文版，第五版目前只有英文版。 DNS必懂基础 DNS主要是用于将域名解析为IP地址的协议，有时候也用于将IP地址反向解析成域名，所以DNS可以实现双向解析。DNS可以使用TCP和UDP的53端口，基本使用UDP协议的53端口。 域的分类 域是分层管理的，就像中国的行政级别。最高层的域是根域(root)”.”，就是一个点，它就像国家主席一样。全球只有13个根域服务器，基本上都在美国，中国一台根域服务器都没有。根域的下一层就是第二层次的顶级域（TLD）了，那么它就是各省省长了。顶级域一般两种划分方法：按国家划分和按组织性质划分。 ◇ 按国家划分：.cn(中国)、.tw(台湾)、.hk(香港)。基本都是两个字母的。◇ 按组织性质划分：.org、.net、.com、.edu、.gov、.cc等。◇ 反向域：arpa。这是反向解析的特殊顶级域。 顶级域下来就是普通的域，公司或个人在互联网上注册的域名一般都是这些普通的域，如baidu.com。 主机名、域名、FQDN 以百度(www.baidu.com)和百度贴吧(tieba.baidu.com)来举例。 ◇ 域名不论是www.baidu.com还是tieba.baidu.com，它们的域名都是baidu.com，严格地说是&quot;baidu.com.&quot;。这是百度所购买的com域下的一个子域名。 ◇ 主机名对于www.baidu.com来说，主机名是www，对于tieba.baidu.com来说，主机名是tieba。其实严格来说，www.baidu.com和tieba.baidu.com才是主机名，它们都是baidu.com域下的主机。一个域下可以定义很多主机，只需配置好它的主机名和对应主机的IP地址即可。 ◇ FQDNFQDN是Fully Qualified Domain Name的缩写，称为完全合格域名，是指包含了所有域的主机名，其中包括根域。FQDN可以说是主机名的一种完全表示形式，它从逻辑上准确地表示出主机在什么地方。 例如www.baidu.com的FQDN是&quot;www.baidu.com.&quot;，com后面还有个点，这是根域；tieba.baidu.com的FQDN是&quot;tieba.baidu.com.&quot;。 域的分层授权 域是从上到下授权的，每一层都只负责自己的直辖下层，而不负责下下层。例如根域给顶级域授权，顶级域给普通域授权，但是根域不会给普通域授权。和现实中的行政管理不一样，域的授权和管理绝对不会向下越级，因为它根本不知道下下级的域名是否存在。 DNS解析流程 以访问www.baidu.com为例。 (1).客户端要访问www.baidu.com，首先会查找本机DNS缓存，再查找自己的hosts文件，还没有的话就找DNS服务器（这个DNS服务器就是计算机里设置指向的DNS）。 (2).DNS服务器收到询问请求，首先查看自己是否有www.baidu.com的缓存，如果有就直接返回给客户端，没有就越级上访到根域&quot;.&quot;，并询问根域。 (3).根域看到是找.com域的，把到.com域的路(地址)告诉DNS服务器，让DNS服务器去找.com询问。 (4).DNS服务器去找.com，”.com”一看是自己辖下的baidu.com，就把baidu.com的IP地址给DNS服务器，让它去找baidu.com。 (5).DNS找到baidu.com，baidu.com发现DNS服务器要找的是自己区域里的www主机，就把这个主机IP地址给了DNS服务器。 (6).DNS服务器把得到的www.baidu.com的IP结果告诉客户端，并缓存一份结果在自己机器中(默认会缓存，因为该服务器允许为客户端递归，否则不会缓存非权威数据)。 (7).客户端得到回答的IP地址后缓存下来，并去访问www.baidu.com，然后www.baidu.com就把页面内容发送给客户端，也就是百度页面。 最后要说明的是： 1.本机查找完缓存后如果没有结果，会先查找hosts文件，如果没有找到再把查询发送给DNS服务器，但这仅仅是默认情况，这个默认顺序是可以改变的。在/etc/nsswitch.conf中有一行” hosts: files dns”就是定义先查找hosts文件还是先提交给DNS服务器的，如果修改该行为”hosts: dns files”则先提交给DNS服务器，这种情况下hosts文件几乎就不怎么用的上了。 2.由于缓存是多层次缓存的，所以真正的查询可能并没有那么多步骤，上图的步骤是完全没有所需缓存的查询情况。假如某主机曾经向DNS服务器提交了www.baidu.com的查询，那么在DNS服务器上除了缓存了www.baidu.com的记录，还缓存了&quot;.com&quot;和&quot;baidu.com&quot;的记录，如果再有主机向该DNS服务器提交ftp.baidu.com的查询，那么将跳过&quot;.&quot;和&quot;.com&quot;的查询过程直接向baidu.com发出查询请求。 /etc/resolv.conf文件 这个文件主要用于定义dns指向，即查询主机名时明确指定使用哪个dns服务器。该文件的详细说明见Linux网络管理之：/etc/resolv.conf。 例如此文件中指定了”nameserver 8.8.8.8”，则每当要查询主机名时，都会向8.8.8.8这台dns服务器发起递归查询，这台dns服务器会帮忙查找到最终结果并返回给你。 当然，在后文的实验测试过程中，使用了另一种方式指定要使用的dns服务器：dig命令中使用”@dns_server”。 DNS术语递归查询和迭代查询 例如A主机要查询C域中的一个主机，A所指向的DNS服务器为B，递归和迭代查询的方式是这样的： 递归查询：A –&gt; B –&gt; C –&gt; B –&gt; A 迭代查询：A –&gt; B A –&gt; C –&gt; A 将递归查询和迭代查询的方式放到查询流程中，就如下图所示。(未标出Client指向的DNS服务器) 也就是说，递归的意思是找了谁谁就一定要给出答案。那么允许递归的意思就是帮忙去找位置，如A对B允许递归，那么B询问A时，A就去帮忙找答案，如果A不允许对B递归，那么A就会告诉B的下一层域的地址让B自己去找。 可以想象，如果整个域系统都使用递归查询，那些公共的根域和顶级域会忙到死，因此更好的方案就是把这些压力分散到每个个人定制的DNS服务器。 所以DNS的解析流程才会如下图。并且在客户端到DNS服务器端的这一阶段是递归查询，从DNS服务器之后的是迭代查询。也就是说，顶级域和根域出于性能的考虑，是不允许给其他任何机器递归的。 为什么客户端到DNS服务器阶段是递归查询？因为客户端本身不是DNS服务器，它自己是找不到互联网上的域名地址的，所以只能询问DNS服务器，最后一定由DNS服务器来返回答案，所以DNS服务器需要对这个客户端允许递归。因此，dns解析器(nslookup、host、dig等)所发出的查询都是递归查询。 权威服务器和(非)权威应答 权威服务器（权威者）可以理解为直接上层域的DNS服务器。例如www.baidu.com这台主机的上层域是baidu.com，那么对www来说，它的权威服务器就是baidu.com这个域内负责解析的DNS服务器，而对于baidu.com这个主机来说，它的权威服务器是.com这个域负责解析的DNS服务器。 更具体的说，某域的权威服务器是可以直接查看该域数据(即区域数据文件)的DNS服务器，主、从DNS服务器都是权威服务器。 只有权威服务器给出的应答才是权威应答，否则就是非权威应答。为什么呢？因为一个域中所有的主机都是在DNS服务器中的区域数据文件中记录的，对于主机来说，它们的位置只有直接上层才知道在哪里。 因此如果解析www.baidu.com时要获得权威应答，应该将DNS指向baidu.com这个域内负责解析的DNS服务器。 只有权威服务器直接给出的答案才是永远正确的，通过缓存得到的答案基本都是非权威应答。当然这不是一定的，因为权威服务器给的答案也是缓存中的结果，但是这是权威答案。DNS服务器缓存解析的数据库时间长度是由权威服务器决定的。 DNS缓存 在Client和DNS服务器这些个人订制的DNS解析系统中都会使用缓存来加速解析以减少网络流量和查询压力，就算是解析不到的否定答案也会缓存。 但是要访问的主机IP可能会改变，所有使用缓存得到的答案不一定是对的，因此缓存给的答案是非权威的，只有对方主机的上一级给的答案才是权威答案。缓存给的非权威答案应该设定缓存时间，这个缓存时间的长短由权威者指定。 另外访问某个域下根本不存在的主机，这个域的DNS服务器也会给出答案，但是这是否定答案，否定答案也会缓存，并且有缓存时间。例如某个Client请求51cto.com域下的ftp主机，但是实际上51cto.com下面可能根本没有这个ftp主机，那么51cto.com就会给否定答案，为了防止Client不死心的访问ftp搞破坏，51cto.com这个域负责解析的DNS服务器有必要给Client指定否定答案的缓存时间。 主、从dns服务器 dns服务器也称为name server，每个域都必须有dns服务器负责该域相关数据的解析。但dns服务器要负责整个域的数据解析，压力相对来说是比较大的，且一旦出现问题，整个域都崩溃无法向外提供服务，这是非常严重的事。所以，无论是出于负载均衡还是域数据安全可用的考虑，两台dns服务器已经是最低要求了，多数时候应该配置多台dns服务器。 多台dns服务器之间有主次之分，主dns服务器称为master，从dns服务器称为slave。slave上的域数据都是从master上获取的，这样slave和master就都能向外提供名称解析服务。 资源记录(Resource Record,RR) 对于提供DNS服务的系统(DNS服务器)，域名相关的数据都需要存储在文件(区域数据文件)中。这些数据分为多种类别，每种类别存储在对应的资源记录(resource record,RR)中。也就是说，资源记录既用来区分域数据的类型，也用来存储对应的域数据。 DNS的internet类中有非常多的资源记录类型。常用的是SOA记录、NS记录、A记录(IPV6则为AAAA记录)、PTR记录、CNAME记录、MX记录等。 (1).SOA记录：start of authority，起始授权机构。该记录存储了一系列数据，若不明白SOA记录，请结合下面的NS记录，SOA更多的信息见”子域”部分的内容。格式如下：123456longshuai.com. IN SOA dnsserver.longshuai.com. mail.longshuai.com. ( 1 3h 1h 1w 1h ) 第四列指定了”dnsserver.longshuai.com.”为该域的master DNS服务器。 第五列是该域的管理员邮箱地址，但注意不能使用@格式的邮箱，而是要将@符号替换为点”.”，正如上面的例子”mail.longshuai.com.”，其实际表示的是”mail@longshuai.com“。 第六列使用括号将几个值包围起来。第一个值是区域数据文件的序列编号serial，每次修改此区域数据文件都需要修改该编号值以便让slave dns服务器同步该区域数据文件。第二个值是刷新refresh时间间隔，表示slave dns服务器找master dns服务器更新区域数据文件的时间间隔。第三个值是重试retry时间间隔，表示slave dns服务器找master dns服务器更新区域数据文件时，如果联系不上master，则等待多久再重试联系，该值一般比refresh时间短，否则该值表示的重试就失去了意义。第四个值是过期expire时间值，表示slave dns服务器上的区域数据文件多久过期。第五个值是negative ttl，表示客户端找dns服务器解析时，否定答案的缓存时间长度。这几个值可以分行写，也可以直接写在同一行中使用空格分开，所以，上面的SOA记录可以写成如下格式： 1longshuai.com. IN SOA dnsserver.longshuai.com. mail.longshuai.com. ( 1 3h 1h 1w 1h ) 前三列是声明性的语句，表示”longshuai.com.”这个域内的起始授权机构为第四列的值”dnsserver.longshuai.com.”所表示的主机。第五列和第六列是SOA的附加属性数据。 每个区域数据文件中都有且仅能有一个SOA记录，且一般都定义为区域数据文件中的资源记录。 注意，资源记录的作用之一是存储域相关的对应数据，所以第4、5、6列表示的是该SOA记录所存储的相关值。 (2).NS记录：name server，存储的是该域内的dns服务器相关信息。即NS记录标识了哪台服务器是DNS服务器。格式如下：1longshuai.com. IN NS dnsserver.longshuai.com. 前三列仍然是声明性语句，表示”longshuai.com.”域内的DNS服务器(name server)为第四列值所表示的”dnsserver.longshuai.com.”主机。 如果一个域内有多个dns服务器，则必然有主次之分，即master和slave之分。但在NS记录上并不能体现主次关系。例如： 12longshuai.com. IN NS dnsserver1.longshuai.com.longshuai.com. IN NS dnsserver2.longshuai.com. 表示主机”dnsserver1.longshuai.com.”和主机”dnsserver2.longshuai.com.”都是域”longshuai.com.”内的dns服务器，但没有区分出主次dns服务器。 很多人搞不懂SOA记录，也很容易混淆SOA和NS记录。其实，仅就它们的主要作用而言，NS记录仅仅只是声明该域内哪台主机是dns服务器，用来提供名称解析服务，NS记录不会区分哪台dns服务器是master哪台dns服务器是slave。而SOA记录则用于指定哪个NS记录对应的主机是master dns服务器，也就是从多个dns服务器中挑选一台任命其为该域内的master dns服务器，其他的都是slave，都需要从master上获取域相关数据。由此，SOA的名称”起始授权机构”所表示的意思也就容易理解了。 (3).A记录：address，存储的是域内主机名所对应的ip地址。格式如下：1dnsserver.longshuai.com. IN A 172.16.10.15 客户端之所以能够解析到主机名对应的ip地址，就是因为dns服务器中的有A记录存储了主机名和ip的对应关系。AAAA记录存储的是主机名和ipv6地址的对应关系。 (4).PTR记录：pointer，和A记录相反，存储的是ip地址对应的主机名，该记录只存在于反向解析的区域数据文件中(并非一定)。格式如下：116.10.16.172.in-addr.arpa. IN PTR www.longshuai.com. 表示解析172.16.10.16地址时得到主机名”www.longshuai.com.&quot;的结果。 (5).CNAME记录：canonical name，表示规范名的意思，其所代表的记录常称为别名记录。之所以如此称呼，就是因为为规范名起了一个别名。什么是规范名？可以简单认为是fqdn。格式如下：1www1.longshuai.com. IN CNAM www.longshuai.com. 最后一列就是规范名，而第一列是规范名即最后一列的别名。当查询”www1.longshuai.com.”，dns服务器会找到它的规范名”www.longshuai.com.&quot;，然后再查询规范名的A记录，也就获得了对应的IP地址并返回给客户端。 CNAME记录非常重要，很多时候使用CNAME可以解决很复杂的问题。而且目前常用的CDN技术有一个步骤就是在dns服务器上设置CNAME记录，将客户端对资源的请求引导到与它同网络环境(电信、网通)以及地理位置近的缓存服务器上。关于CDN的简介，见下文CDN和DNS的关系。 (6).MX记录：mail exchanger，邮件交换记录。负责转发或处理该域名内的邮件。和邮件服务器有关，且话题较大，所以不多做叙述，如有深入的必要，请查看《dns &amp; bind》中”Chapter 5. DNS and Electronic Mail”。 关于资源记录，最需要明确的概念就是它不仅仅用来区分和标识区域数据的类型，还用来存储对应的域数据。 安装阶段请关注下一章！","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"DNS","slug":"DNS","permalink":"http://yoursite.com/tags/DNS/"}]}]}